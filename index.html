<!DOCTYPE html>
    <html lang="en">
      <head>
    <meta charset="utf-8" />

    <title>Deep Learning for Anomaly Detection</title>
    <meta name="description" content="TK" />

    <meta property="og:title" content="Deep Learning for Anomaly Detection" /> 
    <meta property="og:description" content="TK" />
    <meta property="og:image" content="https://experiments.fastforwardlabs.com/log/textflix-report/textflix-report-share.png" />
    <meta property="og:url" content="https://experiments.fastforwardlabs.com/log/textflix-report" />
    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="viewport" content="width=device-width" />
    <link rel="icon" type="image/x-icon" href="/static/images/favicon.png" />
    
    <style type="text/css">
    
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Bold.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Bold.woff') format('woff');
    font-weight: bold;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-BoldItalic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-BoldItalic.woff') format('woff');
    font-weight: bold;
    font-style: italic;
  }
  
    * {
      box-sizing: border-box;
    }
    html {
      background: #fff;
      font-family: "Plex Sans", serif, sans-serif;
      font-size: 17.5px;
      line-height: 28px;
    }
    body {
      margin: 0;
    }
    .content {
      max-width: 64ch;
      padding-left: 2ch;
      padding-right: 2ch;
      margin: 0 auto;
      display: flex;
      flex-direction: column;
      padding-bottom: 0px;
    }
   p, ul, ol {
      margin: 0;
    }
    ul, ol {
      padding-left: 3ch;
    }
  p {
   // text-indent: 3ch;
}
    li p:first-child {
      text-indent: 0;
    }
 
   hr {
      margin: 0;
      border-top-color: black;
      margin-top: -0.5px;
      margin-bottom: 27.5px;
    }
  
h1, h2, h3, h4, h5, h6, button { font-size: inherit; line-height: inherit; font-style: inherit; font-weight: inherit; margin: 0; font-feature-settings: "tnum"; border: none; background: transparent; padding: 0;  }
button:focus, button:hover {
  background: rgba(0,0,0,0.125);
  outline: none;
}
h1 {
  font-size: 42px;
  line-height: 56px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h2 {
  font-size: 31.5px;
  line-height: 42px;
  font-weight: bold;
  margin-top: 28px;
  margin-bottom: 14px;
}
h3 {
  font-size: 26.25px;
  line-height: 35px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h4 {
  font-size: 21px;
  line-height: 28px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h5 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-weight: bold;
}
h6 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-style: italic;
}
p {
  margin-bottom: 14px;
}
figure {
  margin: 0;
  margin-top: 14px;
  margin-bottom: 28px;
}
blockquote {
  margin: 0;
   margin-top: 14px;
  margin-bottom: 14px;
margin-left: 2ch;
}
blockquote + blockquote {
  margin-top: 0;
}
figcaption {
  font-family: "Plex Mono", serif, monospace;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 21px;
}
.info {
  background: #efefef;
  padding-left: 2ch;
  padding-right: 2ch;
  padding-top: 14px;
  padding-bottom: 14px;
  margin-bottom: 28px;
}
.info p:last-child {
  margin-bottom: 0;
}
img {
  display: block;
  max-width: 100%;
  margin: 0 auto;
}

table {
  text-align: left;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 18.900000000000002px;
  border-collapse: collapse;
}
table, th, td {
  border: solid 1px black;
}
td {
  min-width: 18ch;
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  valign: top;
  vertical-align: top;
}
th {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  vertical-align: top;
  background: #efefef;
}
table ul, table ol {
  list-style-position: inside;
  padding-left: 0;
}

  a {
    color: inherit;
  }
  .table-of-contents {
    background: #efefef;
    position: fixed;
    left: 0;
    top: 0;
    width: 32ch;
    height: 100vh;
    overflow-y: auto;
    background: #efefef;
      // background: rgba(230,230,230,0.85);
      //   backdrop-filter: blur(5px);
  }
  body {
    padding-left: 32ch;
  }
  p:empty {
    display: none;
  }

.table-of-contents {
    counter-reset: chapters;
}
 .table-of-contents ul {
    list-style: none;
    padding-left: 0;
  }
 .table-of-contents > ul {
    padding-bottom: 28px;
  }
  .table-of-contents > ul > li > a:before {
          counter-increment: chapters;
          content: counter(chapters) ". ";
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }
 
 .table-of-contents > ul > li > ul > li {
    font-weight: normal;
    font-style: normal;
    text-transform: none;
    letter-spacing: 0;
    margin-left: 0;
  }
 .table-of-contents > ul > li > ul > li > ul > li {
    font-weight: normal;
    font-style: italic;
  }
 .table-of-contents a {
    text-decoration: none;
  }
  .table-of-contents a:hover {
    text-decoration: underline;
  }
 sup {
  }
  .table-of-contents ul a {
    display: block;
    padding-left: 3ch;
    text-indent: -1ch;
    padding-right: 2ch;
  }
  .table-of-contents ul li a.active {
    position: relative;
    background: #ddd;
    // text-decoration: line-through;
  }

 .table-of-contents > ul > li > ul > li > a {
    font-size: 15.75px;
      line-height: 25.2px;
    // padding-left: 4ch;
  }
  .table-of-contents > ul > li > ul > li > ul > li > a {
    padding-left: 5ch;
  }

h1 {
    counter-reset: chp;
}
h2 {
  position: relative;
  padding-top: 42px;
}
  h2:before {
    position: absolute;
    left: 0;
    top: 0;
      font-size: 17.5px;
    color: black;
    counter-increment: chp;
    content: "chapter " counter(chp);
    text-transform: uppercase;
  }
 
  .toc-desktop-hidden .table-of-contents {
    width: auto;
  }
  .toc-desktop-hidden #contents-label {
    display: none;
  }
  .toc-desktop-hidden .table-of-contents ul {
    display: none;
  }
  body.toc-desktop-hidden {
    padding-left: 5ch;
  }
  body:before {
    content: " ";
    height: 28px;
    width: 96ch;
    background: black;
    position: absolute;
    left: 0;
    top: 0;
    z-index: 999;
    display: none;
  }
    #toc-header {
      margin-top: 14px;
      margin-bottom: 14px;
      margin-left: 1ch;
      margin-right: 1ch;
    }
 
  @media screen and (max-width: 1028px) {
    h1 {
      font-size: 36.75px;
      line-height: 49px;
      font-weight: bold;
      margin-top: 14px;
      margin-bottom: 14px;
    }
    .table-of-contents ul li {
      padding-top: 3.5px;
      padding-bottom: 3.5px;
    }

    #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
    }
 
    body {
      padding-left: 0;
      padding-top: 42px;
    }
    #contents-label {
      display: none;
    }
    .table-of-contents {
      height: auto;
      width: 100%;
      z-index: 3;
    }
  body.toc-mobile-show .content:before {
      content: "";
      position: fixed;
      left: 0;
      top: 0;
      bottom: 0;
      right: 0;
      background: rgba(0,0,0,0.25);
      z-index: 2;
      border-top: solid 42px #aaa;
    }
 
    .table-of-contents > ul {
      display: none;
    }
   body.toc-mobile-show {
      overflow: hidden;
    }
    body.toc-mobile-show #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
      position: relative;
    }
    body.toc-mobile-show .table-of-contents {
      width: 32ch;
      height: 100vh;
      max-width: calc(100% - 4ch);
      overflow: auto;
    }
   body.toc-mobile-show .table-of-contents > ul {
      display: block;
      padding-bottom: 28px;
      position: relative;
    }
    body.toc-mobile-show #contents-label {
      display: inline;
      position: relative;
    }
  }
}
</style>
    <script>
    function inViewport(elem) {
      let bounding = elem.getBoundingClientRect();
      return (
        bounding.top >= 0 &&
        bounding.left >= 0 &&
        bounding.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&
        bounding.right <= (window.innerWidth || document.documentElement.clientWidth)
      );
    };

    function setActive(target_id) {
      let selector = '.table-of-contents ul li a[href="#' + target_id + '"]'
      let link = document.querySelector(selector)
      if (link !== null) {
        link.className = 'active'
      }
    }

    window.addEventListener("load", (event) => {
      let headings = document.querySelectorAll('h1, h2, h3, h4');
      let links = document.querySelectorAll('.table-of-contents ul li a')

      observer = new IntersectionObserver((entry, observer) => {
        if (entry[0].intersectionRatio === 1) {
          for (let link of links) {
            link.className = ''
          }
          let target_id = entry[0].target.getAttribute('id')
          setActive(target_id)
        }
      }, { threshold: 1, rootMargin: "0px 0px -50% 0px" });

      let first = true
      for (let heading of headings) {
        if (first && inViewport(heading)) {
          setActive(heading.getAttribute('id'))
          first = false
        }
        observer.observe(heading);
      }

      document.querySelector('#toggle_contents').addEventListener('click', () => {
        let body = document.body
        if (window.innerWidth > 1027) {
          let hidden_class = "toc-desktop-hidden"
          if (body.className === hidden_class) {
            body.className = ''
          } else {
            body.className = hidden_class
          }
        } else {
          let show_class = "toc-mobile-show"
          if (body.className === show_class) {
            body.className = ''
          } else {
            body.className = show_class
          }
        }
      })

      for (let link of links) {
        link.addEventListener('click', (e) => {
          let href = e.target.getAttribute('href')
          let elem = document.querySelector(href)
          window.scroll({
            top: elem.offsetTop - 28,
            left: 0,
            behavior: 'smooth'
          })
          if (window.innerWidth < 1028) {
            document.body.className = ''
          }
          e.preventDefault() 
        })
      }

      document.querySelector('.content').addEventListener('click', () => {
        if (window.innerWidth < 1028) {
          document.body.className = ''
        }
      })
      document.querySelector('.table-of-contents').addEventListener('click', (e) => {
        e.stopPropagation()
      })

      let mediaQueryList = window.matchMedia("(max-width: 1028px)");
      function handleBreakpoint(mql) {
        // clear any left over toggle classes
        document.body.className = ''
      }
      mediaQueryList.addListener(handleBreakpoint);
    }, false);
  </script>


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-140718127-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-140718127-1');
    </script>
  </head>
      <body>
        <div class="content" style="position: relative;">
          <div style="margin-top: 28px;"><a href="https://experiments.fastforwardlabs.com" >Cloudera Fast Forward</a></div>
          <h1 id="deep-learning-for-anomaly-detection">Deep Learning for Anomaly Detection</h1>
<p><div class="table-of-contents"><div id="toc-header" style="display: flex; font-weight: bold; text-transform: uppercase;">
     <div><button id="toggle_contents" style="padding-left: 0.5ch; padding-right: 0.5ch; cursor: pointer; position: relative; top: -1px;">☰</button><span id="contents-label" style="margin-left: 0;"> Contents</span></div>
  </div><ul><li><a href="#introduction">Introduction</a><ul><li><a href="#applications-of-anomaly-detection">Applications of Anomaly Detection</a></li></ul></li><li><a href="#background">Background</a><ul><li><a href="#anomaly-detection-approaches">Anomaly Detection Approaches</a></li><li><a href="#supervised-learning">Supervised learning</a></li><li><a href="#unsupervised-learning">Unsupervised learning</a></li><li><a href="#semi-supervised-learning">Semi-supervised learning</a></li><li><a href="#evaluating-models%3A-accuracy-is-not-enough">Evaluating Models: Accuracy is not Enough</a></li><li><a href="#anomaly-detection-as-learning-normal-behavior">Anomaly Detection as Learning Normal Behavior</a></li><li><a href="#approaches-to-modeling-normal-behavior">Approaches to Modeling Normal Behavior</a></li><li><a href="#why-deep-learning-for-anomaly-detection">Why Deep Learning for Anomaly Detection</a></li><li><a href="#what-can-go-wrong%3F">What can go wrong?</a></li></ul></li><li><a href="#deep-learning-for-anomaly-detection">Deep Learning for Anomaly Detection</a><ul><li><a href="#autoencoders">Autoencoders</a></li><li><a href="#variational-autoencoders">Variational Autoencoders</a></li><li><a href="#generative-adversarial-networks">Generative Adversarial Networks</a></li><li><a href="#sequence-to-sequence-models">Sequence to Sequence Models</a></li><li><a href="#one-class-support-vector-machines">One-Class Support Vector Machines</a></li><li><a href="#additional-considerations">Additional Considerations</a></li><li><a href="#selecting-a-model">Selecting a Model</a></li><li><a href="#general-considerations-in-selecting-a-deep-learning-approach">General Considerations in Selecting a Deep Learning Approach</a></li></ul></li><li><a href="#prototype">Prototype</a><ul><li><a href="#datasets">Datasets</a></li><li><a href="#benchmarking-experiment-setup">Benchmarking Experiment Setup</a></li><li><a href="#web-application-prototypes">Web Application Prototypes</a></li></ul></li><li><a href="#landscape">Landscape</a><ul><li><a href="#open-source-tools-and-frameworks">Open Source Tools and Frameworks</a></li><li><a href="#anomaly-detection-as-a-service">Anomaly Detection as a Service</a></li></ul></li><li><a href="#ethics">Ethics</a><ul><li><a href="#diversity-matters">Diversity Matters</a></li><li><a href="#explainability">Explainability</a></li><li><a href="#additional-use-cases">Additional Use Cases</a></li><li><a href="#innovation">Innovation</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul></div></p>
<figure><img src="figures/cover.png" alt=""></figure>
<p><em>This is an applied research report by Cloudera Fast Forward, focused on Deep Learning Approaches for Anomaly Detection. Read our full report on using deep learning for anomaly detection below or download it (<a href="#">PDF</a>, <a href="#">ePub</a>, <a href="#">mobi</a>). Also be sure to check out our anomaly detection prototypes: <a href="https://blip.fastforwardlabs.com">Blip</a> and <a href="https://anomagram.fastforwardlabs.com">Anomagram</a>.</em></p>
<p>Our reports focus on emerging capabilities that are still changing due to algorithmic breakthrough, hardware breakthrough, technological commoditization, and data availability. Accompanying our reports are working prototypes that exhibit the capabilities of the algorithm and offer detailed technical advice on its practical application.</p>
<h2 id="introduction">Introduction</h2>
<blockquote>
<p>“An outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism”
– Hawkins 1980.</p>
</blockquote>
<p>Anomalies, often referred to as outliers, abnormalities, rare events, or
deviants, are data points or patterns in data that do not conform to a notion of
normal behavior. Anomaly detection, then, is the task of finding those patterns
in data that do not adhere to expected norms, given previous observations.
The capability to recognize or detect anomalous behavior can provide highly
useful insights across industries. Businesses flagging or enacting a planned
response when these unusual cases occur can save them time, costs, and
customers. Hence, anomaly detection has found diverse applications in a variety
of domains: including IT analytics, networks intrusion analytics, medical
diagnostics, financial fraud protection, manufacturing quality control,
marketing and social media analytics, and more.</p>
<h3 id="applications-of-anomaly-detection">Applications of Anomaly Detection</h3>
<p>We’ll begin by taking a closer look at some possible use cases, before diving into different approaches to anomaly detection in the next chapter.</p>
<figure><img src="figures/ill-18.png" alt="Anomaly detection is relevant to several usecases - Network intrusion detection, Medical diagnosis, Fraud detection and manufacturing defect detection."><figcaption>Anomaly detection is relevant to several usecases - Network intrusion detection, Medical diagnosis, Fraud detection and manufacturing defect detection.</figcaption></figure>
<h4 id="network-intrusion-detection">Network Intrusion Detection</h4>
<p>Network security is critical to running a modern viable business, yet all computer
systems suffer from security vulnerabilities which are both technically
difficult and economically punishing to resolve once exploited. Business IT
systems collect data about their own network traffic, user activity in the
system, types of connection requests, etc. While most activity will be benign
and routine, analysis of this data may provide insights into unusual (anomalous)
activity within the network after (and ideally before) a substantive attack. In
practice, the damage and cost incurred right after an intrusion event escalates
faster than most teams are able to respond. Thus, it becomes critical to have
special-purpose intrusion detection systems (IDSs) that surface anomalous
probing and potential threat events early and in a reliable manner.</p>
<h4 id="medical-diagnosis">Medical Diagnosis</h4>
<p>In many medical diagnosis applications, a variety of data points (e.g., x-rays,
MRIs, ECG.) indicative of health status are collected as part of diagnostic
processes. Some of these data points are also collected by end-user medical
devices (e.g., glucose monitors, pacemakers, smart watches). Anomaly detection
approaches can be applied to highlight situations of abnormal readings which may
be indicative of health conditions or precursor signals of medical incidents.</p>
<h4 id="fraud-detection">Fraud Detection</h4>
<p>In 2018, fraud was globally estimated to cost &gt; $5 Trillion USD (&gt; £3.89
Trillion GBP). Within the financial service industry, it is critical for service
providers to correctly identify and react to fraudulent transactions. In the
most straightforward cases, a transaction may be identified as fraudulent
relative to the historical transactions by a given party or by comparison to all
other transactions occurring within the same time period for a peer group. Here,
fraud can be cast as a deviation from normal transaction data and addressed
using anomaly detection approaches. Even as financial fraud is further clustered
into card-based, check-based, unauthorized account access, or authorized
payment-based categories, the core concepts of baselining an individual’s
standard behavior and looking for signals of unusual activity applies.</p>
<h4 id="manufacturing-defect-detection">Manufacturing Defect Detection</h4>
<p>Within the manufacturing industry, an automated approach to the task of
detecting defects in large-volume manufactured items is vital to quality
assurance. This task can be cast as anomaly detection where the goal is to
identify manufactured items that significantly or even slightly differ from
ideal (normal) items that have passed quality assurance tests. The amount of
acceptable deviation is determined by the company and customers, as well as
industry and regulatory standards.</p>
<div class="info">
<p>Use Case Example:</p>
<p>The ScoleMans have been making drywall panels and are the leading regional
supplier for construction companies. Occasionally, some of the wall panels they
produce have defects - cracks, chips at edges, paint/coating issues, etc. As
part of their QA process, they have both RGB and thermal images of each produced
panel which let their QA engineers flag defective units. They want to automate
this process - i.e., develop tools that automatically identify these defects.
ScoleMans can use a deep anomaly detection model for identifying these defects.
In particular, they can use an AutoEncoder or GAN-based model built with
convolutional neural network blocks (see &lt;&lt;Chapter 3: Technical&gt;&gt; for more
information) to create a model of normal data based on images of normal walls.
This model can then be used to tag new wall images as normal or abnormal, and
set thresholds for alerts.</p>
</div>
<p>Similarly, the task of predictive maintenance can be cast as anomaly detection.
For example, anomaly detection approaches can be applied to data from machine
sensors (vibrations, temperature, drift, etc.) where abnormal sensor readings
can be indicative of impending failures.</p>
<p>Thus we see anomaly detection is a significant problem faced in several areas.
Detecting and correctly classifying something unseen as anomalous is a
challenging problem that has been tackled in many different manners over the
years. While there are many approaches, the traditional machine learning
approaches are sub-optimal when it comes to high dimensional data and sequence
datasets since it fails to capture the complex structure in the data.</p>
<p>This report, with its accompanying prototype, explores deep learning-based
approaches that first learn to model normal behavior and then exploit this
knowledge to identify anomalies. While they can yield remarkable results on
complex and high dimensional data, there are several factors that influence the
choice of approach when building an anomaly detection application. In this
report we survey the approaches, highlighting their pros and cons.</p>
<h2 id="background">Background</h2>
<p>In this chapter, we provide an overview of approaches to anomaly detection based
on the type of data available, how to evaluate an anomaly detection model, how
each approach constructs a model of normal behavior and why deep learning
models are valuable. It concludes with a discussion of pitfalls that may occur
while deploying these models.</p>
<h3 id="anomaly-detection-approaches">Anomaly Detection Approaches</h3>
<p>Anomaly detection approaches can be categorized in terms of the type of data
needed to train an anomaly detection model. Within most use cases, it is
expected that anomalous samples represent a very small percentage of the entire
dataset. Thus, even when available data is labeled, normal data samples are more
readily available compared to abnormal samples. This assumption is critical for
most  applications today.  In the following sections, we touch on how the
availability of labeled data impacts the choice of approach.</p>
<h3 id="supervised-learning">Supervised learning</h3>
<p>When learning with supervision, machines rely on examples that illustrate the
relationship between the input features and output. The goal of supervised
anomaly detection algorithms is to incorporate application-specific knowledge
into the anomaly detection process. With sufficient normal and anomalous
examples, the anomaly detection task can be reframed as a classification task
where the machines can learn to accurately predict whether a given example is an
anomaly or not.  That said, for many anomaly detection use cases the proportion
of normal versus anomalous examples is highly imbalanced. And while there may be
multiple anomalous classes, each of them could be quite under-represented.</p>
<figure><img src="figures/ill-16.png" alt="An illustration of supervised learning"><figcaption>An illustration of supervised learning</figcaption></figure>
<p>This approach assumes we have labeled examples for all types of anomalies that
could occur and can correctly classify them. In practice, this is usually not
the case, as anomalies can take many different forms, with novel anomalies
emerging at test time. Thus, approaches that generalize well and are more
effective at identifying previously unseen anomalies are preferable.</p>
<h3 id="unsupervised-learning">Unsupervised learning</h3>
<p>When it comes to unsupervised approaches, one does not possess examples that
illustrate the relationship between input features and output. Instead, in this
case, machines learn by finding structure within the input features. Owing to
the frequent lack of labeled anomalous data, unsupervised approaches are more
popular than supervised ones in the anomaly detection field. That said, the
nature of the anomalies one hopes to detect is often highly specific. Thus,
many of the anomalies found in a completely unsupervised manner could correspond
to noise, and may not be of any interest to the business.</p>
<figure><img src="figures/ill-17.png" alt="An illustration of unsupervised learning"><figcaption>An illustration of unsupervised learning</figcaption></figure>
<h3 id="semi-supervised-learning">Semi-supervised learning</h3>
<p>Semi-supervised learning falls between supervised and unsupervised learning
approaches. It includes a set of methods that take advantage of large amounts of
unlabeled data as well as small amounts of labeled data.  Many real world
anomaly detection use cases nicely fit this criteria, in the sense that there
are a huge number of normal examples available but the more unusual or abnormal
classes of interest are insufficient to be effectively learned from. Following
the assumption that most data points within an unlabeled dataset are normal, we
can train a robust model on an unlabeled dataset and evaluate its skill (as well
as  tune the model’s parameters) using a small amount of labeled data.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> For
instance, in a network intrusion detection application, one may have examples of
the normal class and some examples of the intrusion classes, but new kinds of
intrusions may often arise with time.</p>
<figure><img src="figures/ill-20.png" alt="An illustration of semi-supervised learning"><figcaption>An illustration of semi-supervised learning</figcaption></figure>
<p>To give another example, in the case of border security or X-ray screening for
aviation, anomalous items posing a security threat are not commonly encountered.
Exemplary data of anomalies can be difficult to obtain in any quantity, since no
such events may have occurred in the first place. In addition, the nature of any
anomaly posing a potential threat may evolve due to a range of external factors.</p>
<figure><img src="figures/ill-19.png" alt="Exemplary data in certain applications can be difficult to obtain"><figcaption>Exemplary data in certain applications can be difficult to obtain</figcaption></figure>
<p>Such situations may require the determination of both abnormal classes as well
as novel classes, for which little to no labeled data is available. In cases like these,
a semi-supervised classification approach that enables detection of both known and
previously unseen anomalies is an ideal solution.</p>
<h3 id="evaluating-models%3A-accuracy-is-not-enough">Evaluating Models: Accuracy is not Enough</h3>
<p>As mentioned earlier in <a href="#anomaly-detection-approaches">Anomaly Detection Approaches</a>,
it is expected that the distribution between the normal and abnormal class(es)
can be very skewed. This is commonly referred to as <strong>class imbalance</strong>.
A model that learns from such data may not be robust: it may be accurate when
classifying examples within the normal class, but perform poorly when
classifying anomalous examples.</p>
<p>For example, consider a dataset of 1000 images of luggage that go through a
security checkpoint. 950 images are of normal luggage and 50 are abnormal.
Assuming we have an unskilled model which simply classifies <strong><em>all</em></strong> images as normal, it can still achieve high overall accuracy for this dataset (95% - i.e,. 95% for normal data and 0% for
abnormal data).</p>
<p>Such a model may also misclassify normal examples as anomalous (<strong>false positives,
FP</strong>), or misclassify anomalous examples as normal ones (<strong>false negatives, FN</strong>).  As we consider both of these types of errors, it becomes obvious that the
traditional accuracy metric (total number of correct classifications divided by
total classifications) is insufficient in evaluating the skill of an anomaly
detection model.</p>
<p>Two important metrics have been introduced that provide a better measure of
model skill:  precision and recall. Precision is defined as the number of true
positives (TP) divided by the number of true positives (TP) plus the number of
false positives (FP), while recall is the number of true positives (TP) divided
by the number of true positives (TP) plus the number of false negatives (FN).
Depending on the use case or application, it may be desirable to optimize for
either precision or recall.</p>
<p>Optimizing for precision may be useful when the cost of failure is low, or to
reduce human workload. Optimising for high recall may be more appropriate when
the cost of a false negative is very high (e.g., airport security, where it is
better to flag many items for human inspection in an image (low cost) in order
to avoid the cost of incorrectly admitting a dangerous item on a flight). While
there are several ways to optimize for precision or recall, the manner in which
a threshold is set can be used to reflect the precision and recall preferences
for each specific use case.</p>
<div class="info">
<p>So far, we have reviewed reasons why a semi-supervised
approach to anomaly detection is desirable, and explored robust metrics for
evaluating these models. In the next section, we focus on these semi-supervised
approaches and discuss how they work.</p>
</div>
<h3 id="anomaly-detection-as-learning-normal-behavior">Anomaly Detection as Learning Normal Behavior</h3>
<p>The underlying strategy for most approaches to anomaly detection is to first
model normal behavior, and then exploit this knowledge in identifying deviations
(anomalies).  This approach typically falls under the semi-supervised category
and is accomplished across two steps in the anomaly detection loop. The first
step, which we can refer to as the training step, involves building a model of
normal behavior using available data. Depending on the specific anomaly
detection method, this training data may contain both normal and abnormal data
points or only normal data points (see <a href="#deep-learning-for-anomaly-detection">Chapter 3:
Deep Learning for Anomaly Detection</a> for additional
details on AD methods).  Based on this model, an anomaly score is then assigned
to each data point that represents a measure of deviation from normal behavior.</p>
<figure><img src="figures/ill-13.png" alt="Illustration shows the training phase in the anomaly detection loop. Based ondata (which may or may not contain abnormal samples), the AD model learns amodel of normal behavior and assigns an anomaly score based onthis."><figcaption>Illustration shows the training phase in the anomaly detection loop. Based on
data (which may or may not contain abnormal samples), the AD model learns a
model of normal behavior and assigns an anomaly score based on
this.</figcaption></figure>
<figure><img src="figures/ill-14.png" alt="Illustration of the test step in the anomaly detectionloop."><figcaption>Illustration of the test step in the anomaly detection
loop.</figcaption></figure>
<p>The second step in the anomaly detection loop, the test step, introduces the
concept of threshold-based anomaly tagging. Given the range of scores assigned
by the model, we can select a threshold rule that drives the anomaly tagging
process - e.g., scores above a given threshold are tagged as anomalies, while
those below it are tagged as normal. The idea of a threshold is valuable, as it
provides the analyst some easy lever to tune the “sensitivity” of the anomaly
tagging process. Interestingly, while most methods for anomaly detection follow
this general approach, they differ in how they model normal behavior and
generate anomaly scores.</p>
<figure><img src="figures/ill-15.png" alt="Anomaly scoring"><figcaption>Anomaly scoring</figcaption></figure>
<p>To further illustrate this process, consider the scenario where the task is to
detect abnormal temperatures (e.g., spikes), given data from the temperature
sensor attached to servers in a data center. We can use a statistical approach
(see table in section <a href="#approaches-to-modeling-normal-behavior">Approaches to Modeling Normal
Behavior</a> for an overview of common methods).
In step 1, we assume the samples follow a normal distribution,
and we can use sample data to learn the parameters of this distribution (mean
and variance). We can assign an anomaly score based on a sample’s deviation from
the mean and set a threshold (e.g., any value with more than 3 standard
deviations from the mean is an anomaly).  In step 2, we then tag all new
temperature readings and generate a report.</p>
<h3 id="approaches-to-modeling-normal-behavior">Approaches to Modeling Normal Behavior</h3>
<p>Given the importance of the anomaly detection task, multiple approaches have
been proposed and rigorously studied over the last few decades. To provide a
high level summary, we categorize the more popular techniques into four main
areas: clustering, nearest neighbour, classification, and statistical.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>
The Table below provides a summary of examples, assumptions, and anomaly scoring
strategies taken by approaches within each category.</p>
<div style="width: 100%; overflow-x: auto;"><table>
<thead>
<tr>
<th>AD Method</th>
<th>Assumptions</th>
<th>Anomaly Scoring</th>
<th>Notable Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Clustering</td>
<td>Normal data points belong to a cluster (or lie close to its centroid) in the data while anomalies do not belong to any clusters</td>
<td>Distance from nearest cluster centroid</td>
<td>Self Organising Maps (SOM), K-Means Clustering, Expectation Maximization (EM)</td>
</tr>
<tr>
<td>Nearest Neighbour</td>
<td>Normal data instances occur in dense neighborhoods while anomalous data are far from their nearest neighbors</td>
<td>Distance from Kth nearest neighbour</td>
<td>KNN</td>
</tr>
<tr>
<td>Classification</td>
<td><ul><li>A classifier can be learned which distinguishes between normal and anomalous with the given feature space</li><li>Labeled data exists for normal and abnormal data</li></ul></td>
<td>A measure of classifier estimate (likelihood) that a data point belongs to the normal class</td>
<td>One Class SVM, Autoencoders, Sequence to Sequence Models</td>
</tr>
<tr>
<td>Statistical</td>
<td>Given an assumed stochastic model, normal data falls in high probability regions of the model while abnormal data lie in low probability regions</td>
<td>Probability that datapoint lies a high  probability region in the assumed distribution</td>
<td>Regression Models (ARMA, ARIMA), Gaussian Models, GANs, VAEs</td>
</tr>
</tbody>
</table></div>
<p>The majority of these approaches have been applied to univariate time series
data -  a single datapoint generated by the same process at various time steps
(e.g., readings from a temperature sensor over time) and assume linear
relationships within the data. Examples include KNNs, K-Means Clustering, ARMA,
ARIMA, etc. However, data is increasingly high dimensional (e.g., multivariate
datasets, images, videos) and the detection of anomalies may require the joint
modeling of interactions between each variable. For these sorts of problems,
deep learning approaches (the focus of this report) such as Autoencoders,
Variational Autoencoders, Sequence to Sequence Models, and Generative
Adversarial Networks present some benefits.</p>
<h3 id="why-deep-learning-for-anomaly-detection">Why Deep Learning for Anomaly Detection</h3>
<p>Deep learning approaches, when applied to anomaly detection, offer several
advantages.</p>
<ul>
<li>
<p><strong>Multivariate, high dimensional data</strong>: Deep learning approaches are designed to work with multivariate data of
different data types across each variable. This makes it easy to integrate
information from multiple sources; it also eliminates challenges associated
with individually modeling anomaly for each variable and aggregating the
results.</p>
</li>
<li>
<p><strong>Modeling interaction between variables</strong>: Deep learning approaches work well in jointly modeling the interactions between
multiple variables with respect to a given task. In addition, beyond the
specification of generic hyperparameters (number of layers, units per layer,
etc.), deep learning models require minimal tuning to achieve good results.</p>
</li>
<li>
<p><strong>Performance</strong>: Deep learning methods offer the opportunity to model complex, non-linear
relationships within data, and leverage this for the anomaly detection task. The
performance of deep learning models can also potentially scale with the
availability of appropriate training data, making them suitable for data-rich
problems.</p>
</li>
</ul>
<p>NM: We need to move this somewhere else
RH: That may not be the right link to use for the Interpretability report.</p>
<ul>
<li><strong>Interpretability</strong>: While deep learning methods can be complex (leading to their reputation as black
box models), interpretability techniques such as LIME (see our previous report:
&lt;<Interpretability>&gt;) and Deep SHAP<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> provide opportunities to inspect
their behavior and make them more interpretable by analysts.</li>
</ul>
<h3 id="what-can-go-wrong%3F">What can go wrong?</h3>
<p>There is a proliferation of algorithmic approaches that can help tackle an
anomaly detection task and allow us to build solid models, at times even with
just normal samples. But what is the catch? Do they really work? What could
possibly go wrong?</p>
<ul>
<li>
<p><strong>Contaminated normal examples</strong>: In large scale applications that have huge volumes of data, it is quite possible
that the large unlabeled data is considered as the normal class wherein a small
percentage of examples may actually be anomalous or simply be poor training
examples. And while some models (like one-class SVM or isolation forest) can
account for this, there are others that may not be robust to detecting
anomalies.</p>
</li>
<li>
<p><strong>Computational complexity</strong>: Anomaly detection scenarios can sometimes have low-latency requirements i.e the
ability to  speedily retrain existing models as new data becomes available and
perform inference. This can be computationally expensive at scale, even for
linear models on univariate data. Deep learning models, also incur
additional compute costs to estimate their large number of parameters. To
address these compute issues, it is recommended to explore tradeoffs which
balance the frequency of retraining and overall accuracy.</p>
</li>
<li>
<p><strong>Human supervision</strong>: One major challenge with unsupervised and semi-supervised approaches is that
they can be noisy and may generate a large amount of false positives. In turn,
false positives incur labour costs associated with human review. Given these
costs, an important goal for anomaly detection systems is to incorporate the
results of human review (as labels) in improving model quality.</p>
</li>
<li>
<p><strong>Definition of anomaly</strong>: The boundary between normal and anomalous behavior is often not precisely
defined in several data domains and is continually evolving. Unlike other task
domains where dataset shift occurs sparingly, the anomaly detection systems
should anticipate (frequent) and account for changes in the distribution of the
data. In many cases, this can be achieved by frequent retraining of models.</p>
</li>
<li>
<p><strong>Threshold selection</strong>: The process of selecting a good threshold value can be challenging. In a
semi-supervised setting (the approaches covered above), one has access to a pool
of labeled data. Using these labels (and some domain expertise), we can
determine a suitable threshold. Specifically, we can explore the range of
anomaly scores for each data point in the validation set and select a threshold
as the point that yields the best performance metric (accuracy, precision,
recall). In the absence of labeled data, and if we assume that most data points
are normal, we can use statistics such as standard deviation and percentiles to
infer a good threshold.</p>
</li>
</ul>
<h2 id="deep-learning-for-anomaly-detection-2">Deep Learning for Anomaly Detection</h2>
<p>As data becomes high dimensional, it is increasingly challenging to effectively
learn a model of normal behavior. In this chapter, we will review a set of relevant
deep learning model architectures and
how they can be applied to the task of anomaly detection. As discussed in
<a href="#background">Chapter 2. Background</a>, anomaly detection involves first learning a model
of normal behavior, then generating anomaly scores that can be used to identify anomalous activity.</p>
<p>The deep learning approaches discussed here typically consist of two principal
components: an encoder that learns to generate an internal representation of
the input data, and a decoder that attempts to reconstruct the original input
based on this internal representation. While the exact techniques for encoding
and decoding vary across models, the overall benefit they offer is the ability
to learn the distribution of normal input data and construct a measure of
anomaly respectively.</p>
<h3 id="autoencoders">Autoencoders</h3>
<p>Autoencoders are neural networks designed to learn a low-dimensional
representation, given some input data. They consist of two components: an
encoder  that learns to map input data to a low-dimensional representation
(termed the <em>bottleneck</em>), and a decoder that learns to map this low-dimensional
representation back to the original input data. By structuring the learning
problem in this manner, the encoder network learns an efficient “compression”
function that maps input data to a salient lower-dimensional representation, such
that the decoder network is able to successfully reconstruct the original input
data. The model is trained by minimizing the reconstruction error, which is the
difference (mean squared error) between the original input and the reconstructed
output produced by the decoder. In practice, autoencoders have been applied as a
dimensionality reduction technique, as well as in other use cases such as
noise removal from images, image colorization, unsupervised feature extraction and
data compression.</p>
<figure><img src="figures/ill-1.png" alt="The components of an autoencoder"><figcaption>The components of an autoencoder</figcaption></figure>
<p>It is important to note that the mapping function learned by an autoencoder is
specific to the training data distribution. That is, an autoencoder will typically
not succeed at reconstructing data that is significantly different from the data it
has seen during training. As we will see later in this chapter, this property of
learning a distribution-specific mapping (as opposed to a generic linear
mapping) is particularly useful for the task of anomaly detection.</p>
<h4 id="modeling-normal-behavior-and-anomaly-scoring">Modeling Normal Behavior and Anomaly Scoring</h4>
<p>Applying an autoencoder for anomaly detection follows the general principle of
first modeling normal behavior and subsequently generating an anomaly score for
each new data sample. To model normal behavior, we follow a semi-supervised
approach where we train the autoencoder on normal data samples. This way, the
model learns a mapping function that successfully reconstructs normal data
samples with a very small reconstruction error. This behavior is replicated
at test time, where the reconstruction error is small for normal data samples,
and large for abnormal data samples. To identify anomalies, we use the
reconstruction error score as an anomaly score and flag samples with
reconstruction errors above a given threshold.</p>
<figure><img src="figures/ill-2.png" alt="The use of autoencoders for anomaly detection."><figcaption>The use of autoencoders for anomaly detection.</figcaption></figure>
<p>This process is illustrated in the following figure. As
the autoencoder attempts to reconstruct abnormal data, it does so in a manner
that is weighted toward normal samples (square shapes). The difference between
what it reconstructs and the input is the reconstruction error. We can specify a
threshold and flag anomalies as samples with a reconstruction error above the
given threshold.</p>
<h3 id="variational-autoencoders">Variational Autoencoders</h3>
<p>A variational autoencoder (VAE) is an extension of the autoencoder. Similar to
an autoencoder, it consists of an encoder and a decoder network component,
but it also includes important changes in the structure of the learning problem to
accommodate variational inference. As opposed to learning a mapping from the input
data to a fixed bottleneck vector (a point estimate), a VAE learns a mapping
from input to a distribution, and learns to reconstruct the original data by
sampling from this distribution using a latent code. In Bayesian terms, the
prior is the distribution of the latent code, the likelihood is the distribution
of the input given the latent code, and the posterior is the distribution of the
latent code, given our input. The components of a VAE serve to derive good
estimates for these terms.</p>
<p>The encoder network learns the parameters (mean and variance) of a distribution
that outputs a latent code vector, given the input data (posterior). In other
words, one can draw samples of the bottleneck vector that “correspond” to samples
from the input data. The nature of this distribution can vary depending on the
nature of the input data (e.g., while Gaussian distributions are commonly used,
Bernoulli distributions can be used if the input data is known to be binary).<br>
On the other hand, the decoder learns a distribution that outputs the original
input data point (or something really close to it), given a latent bottleneck
sample (likelihood). Typically, an isotropic Gaussian distribution is used to
model this reconstruction space.</p>
<p>The VAE model is trained by minimizing the difference between the estimated
distribution produced by the model and the real distribution of the data. This
difference is estimated using the Kullback-Leibler divergence, which quantifies
the distance between two distributions by measuring how much information is lost
when one distribution is used to represent the other. Similar to autoencoders, VAEs have
been applied in use cases such as unsupervised feature extraction,
dimensionality reduction, image colorization, image denoising, etc. In addition,
given that they use model distributions, they can be leveraged for controlled
sample generation.</p>
<p>The probabilistic Bayesian components introduced in VAEs lead to a few useful
benefits. First, VAEs enable Bayesian inference; essentially, we can now sample
from the learned encoder distribution and decode samples that do not explicitly
exist in the original dataset, but belong to the same data distribution. Second,
VAEs learn a disentangled representation of a data distribution�i.e., a single
unit in the latent code is only sensitive to a single generative factor. This
allows some interpretability of the output of VAEs, as we can vary units in the
latent code for controlled generation of samples. Third, a VAE provides true
probability measures that offer a principled approach to quantifying
uncertainty when applied in practice (for example, the probability that a new data
point belongs to the distribution of normal data is 80%).</p>
<figure><img src="figures/ill-3.png" alt="A variational autoencoder"><figcaption>A variational autoencoder</figcaption></figure>
<h4 id="modeling-normal-behavior-and-anomaly-scoring-2">Modeling Normal Behavior and Anomaly Scoring</h4>
<p>Similar to an autoencoder, we begin by training the VAE on normal data samples.
At test time, we can compute an anomaly score in two ways. First, we can draw
samples of the latent code z from the encoder given our input data, sample
reconstructed values from the decoder using z, and compute a mean reconstruction
error. Anomalies are flagged based on some threshold on the reconstruction
error.</p>
<p>Alternatively, we can output a mean and a variance parameter from the decoder,
and compute the probability that the new data point belongs to the distribution
of normal data on which the model was trained. If the data point lies in a
low-density region (below some threshold), we flag that as an anomaly. We can
do this because we’re modeling a distribution as opposed to a point estimate.</p>
<figure><img src="figures/ill-4.png" alt="Two approaches to anomaly scoring with a VAE: we have the option of outputting the meanreconstruction probability (i.e., the probability that a sample belongs to thenormal data distribution)."><figcaption>Two approaches to anomaly scoring with a VAE: we have the option of outputting the mean
reconstruction probability (i.e., the probability that a sample belongs to the
normal data distribution).</figcaption></figure>
<h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3>
<p>Generative adversarial networks (GANs) are neural networks designed to learn a generative model of an input data
distribution. In their classic formulation, they’re composed of a pair of
(typically feed-forward) neural networks termed a generator, G, and discriminator, D.
Both networks are trained jointly and play a competitive skill game with the end
goal of learning the distribution of the input data, X.</p>
<p><strong>NM: we use “images” instead of “input datapoints or samples” below - check</strong>
The generator network G learns a mapping from random noise of a fixed dimension
(Z) to samples X_ that closely resemble members of the input data distribution.
The discriminator D learns to correctly discern real samples that originated
in the source data (X) from fake samples (X_) that are generated by G. At each
epoch during training, the parameters of G are updated to maximize its
ability to generate samples that are indistinguishable by D, while the
parameters of D are updated to maximize its ability to to correctly discern
true samples X from generated samples X_. As training progresses, G becomes
proficient at producing samples that are similar to X, and D also upskills on
the task of distinguishing real from fake samples.</p>
<p>In this classic formulation of GANs, while G learns to model the source
distribution X well (it learns to map random noise from Z to the source
distribution), there is no straightforward approach that allows us to harness
this knowledge for controlled inference�i.e., to generate a sample that is
similar to a given known sample. While we can conduct a broad search over the
latent space with the goal of recovering the most representative latent noise
vector for an arbitrary sample, this process is compute-intensive and very slow
in practice.</p>
<p>To address these issues, recent research studies have explored new formulations
of GANs (known as BiGANs) that enable just this sort of controlled adversarial inference by
introducing an encoder network, E.^[[See e.g. Jeff Donahue et al., “Adversarial Feature Learning” (2016), <a href="https://arxiv.org/abs/1605.09782">arXiv:1605.09782</a>, and Samet AkCay et al., “GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training” (2018), <a href="https://arxiv.org/abs/1805.06725">arXiv:1805.06725</a>.] In simple terms, the encoder learns the
reverse mapping of the generator; it learns to generate a fixed vector Z_,
given a sample. Given this change, the input to the discriminator is also
modified; the discriminator now takes in pairs of input that include the latent
representation (Z, and Z_), in addition to the data samples (X and X_). The
encoder E is then jointly trained with the generator G; G learns an induced
distribution that outputs samples of X given a latent code z, while E learns an
induced distribution that outputs Z, given a sample X.</p>
<figure><img src="figures/ill-5.png" alt="A traditional GAN (A) and a BiGAN (B)."><figcaption>A traditional GAN (A) and a BiGAN (B).</figcaption></figure>
<p>Again, the mappings learned by components in the GAN are specific to the
data used in training. For example, the generator component of a GAN trained on
images of cars will always output an image that looks like a car, given any
latent code. At test time, we can leverage this property to infer how different
a given input sample is from the data distribution on which the model was
trained.</p>
<h4 id="modeling-normal-behavior-and-anomaly-scoring-3">Modeling Normal Behavior and Anomaly Scoring</h4>
<p>To model normal behavior, we train a BiGAN on normal data samples. At the end
of the training process, we have an encoder E that has learned a mapping from
data samples (X) to latent code space (Z_), a discriminator D that has learned to
distinguish real from generated data, and a Generator G that has learned a
mapping from latent code space to sample space. Note that these mappings are
specific to the distribution of normal data that has been seen during training.
At test time, we perform the following steps to generate an anomaly score for a
given sample X. First, we obtain a latent space value z from the encoder given
X, which is fed to the generator and yields a sample X_. Next, we can compute an
anomaly score based on the reconstruction loss (difference between X and X_) and
the discriminator loss (cross entropy loss or feature differences in the last
dense layer of the discriminator, given both X and X_).</p>
<figure><img src="figures/ill-7.png" alt="A BiGAN applied to the task of anomaly detection."><figcaption>A BiGAN applied to the task of anomaly detection.</figcaption></figure>
<h3 id="sequence-to-sequence-models">Sequence to Sequence Models</h3>
<p>Sequence to sequence models are a class of neural networks mainly designed to
learn mappings between data that are best represented as sequences. Data
containing sequences can be challenging as each token in a sequence may have
some form of temporal dependence on other tokens�a relationship that has to be
modeled to achieve good results. For example, consider the task of language
translation where a sequence of words in one language needs to be mapped to a
sequence of words in a different language. To excel at such a task, a model must
take into consideration the (contextual) location of each word/token within the
broader sentence; this allows it to generate an appropriate translation (See our
previous report on <a href="https://blog.fastforwardlabs.com/2019/07/17/new-research-transfer-learning-for-natural-language-processing.html">Natural Language Processing</a> to learn more about this area.)</p>
<p>On a high level, sequence to sequence models typically consist of an encoder, E,
that generates a hidden representation of the input tokens, and a decoder, D,
that takes in the encoder representation and sequentially generates a set of
output tokens. Traditionally, the encoder and decoder are composed of long short-term memory (LSTM)
blocks, that are particularly suitable for modeling temporal relationships
within input data tokens.</p>
<p><strong>// RH: with the total number of steps determining the length of the output token? Or is it the reverse?</strong>
While sequence to sequence models excel at modeling data with temporal
dependence, they can be slow during inference�each individual token in the
model output is sequentially generated at each time step, where the total number
of steps is the length of the output token).</p>
<p>We can use this encoder/decoder structure for anomaly detection by revising the
sequence to sequence model to function like an autoencoder, training the
model to output the same tokens as the input, shifted by 1. This way, the
encoder learns to generate a hidden representation that allows the decoder to
reconstruct input data that is similar to examples seen in the training dataset.</p>
<figure><img src="figures/ill-8.png" alt="A sequence to sequence models"><figcaption>A sequence to sequence models</figcaption></figure>
<h4 id="modeling-normal-behavior-and-anomaly-scoring-4">Modeling Normal Behavior and Anomaly Scoring</h4>
<p>To identify anomalies, we take a semi-supervised approach where we train the
sequence to sequence model on normal data. At test time, we can then compare the
difference (mean squared error) between the output sequence generated by the model and
its input. As in the approaches discussed previously, we can use this value as
an anomaly score.</p>
<h3 id="one-class-support-vector-machines">One-Class Support Vector Machines</h3>
<p>In this section, we discuss <em>one-class support vector machines</em> (OCSVMs), a
non-deep learning approach to classification that we will use later (see
<a href="#prototype">Chapter 4. Prototype</a>) as a baseline.</p>
<p>Traditionally, the goal of classification approaches is to help distinguish
between different classes, using some training data. However, consider a
scenario where we have data for only one class, and the goal is to determine
whether test data samples are similar to the training samples.
OCSVMs were introduced for exactly this sort of task: <em>novelty detection</em>, or the
detection of unfamiliar samples. SVMs have proven very popular for classification, and they
introduced the use of kernel functions to create nonlinear decision boundaries
(hyperplanes) by projecting data into a higher dimension. Similarly, OCSVMs
learn a decision function which specifies regions in the input data space where
the probability density of the data is high. An OCSVM model is trained with various
hyperparameters:</p>
<ul>
<li><code>nu</code> specifies the fraction of outliers (data samples
that do not belong to our class of interest) that we expect in our data.</li>
<li><code>kernel</code> specifies the kernel type to be used in the algorithm; examples include RBF, polynomial (poly), and linear. This enables SVMs to use a nonlinear function to project the input data to a higher dimension.</li>
<li><code>gamma</code> is a parameter of the RBF kernel type that controls the influence of
individual training samples; this affects the “smoothness” of the model.</li>
</ul>
<figure><img src="figures/ill-9.png" alt="An OCSVM classifier learns a decision boundary around data seen duringtraining."><figcaption>An OCSVM classifier learns a decision boundary around data seen during
training.</figcaption></figure>
<h4 id="modeling-normal-behavior-and-anomaly-scoring-5">Modeling Normal Behavior and Anomaly Scoring</h4>
<p>To apply OCSVM for anomaly detection, we train an OCSVM model using normal data,
or data containing a small fraction of abnormal samples. Within most implementations of OCSVM,
the model returns an estimate of how similar a data point is to the data samples
seen during training. This estimate may be the distance from the decision
boundary (the separating hyperplane), or a discrete class value (+1 for data that
is similar and -1 for data that is not). Either type of score can be used as an
anomaly score.</p>
<figure><img src="figures/ill-10.png" alt="At test time, An OCSVM model classifies data points outside the learneddecision boundary as anomalies (assigned class of -1)."><figcaption>At test time, An OCSVM model classifies data points outside the learned
decision boundary as anomalies (assigned class of -1).</figcaption></figure>
<h3 id="additional-considerations">Additional Considerations</h3>
<p><strong>// RH: Possible to add some text between these headings?</strong></p>
<h4 id="anomalies-as-rare-events">Anomalies as Rare Events</h4>
<p>For the training approaches discussed thus far, we operate on the assumption of the
availability of “normal” labeled data, which is then used to learn a model of
normal behavior. In practice, it is often the case that labels do not exist or
can be expensive to obtain. However, it is also a common observation that
anomalies (by definition) are relatively infrequent events and therefore
constitute a small percentage of the entire event dataset (for example, the occurrence
of fraud, machine failure, cyberattacks, etc.). Our experiments (see
<a href="#prototype">Chapter 4. Prototype</a> for more discussion) have shown that the neural network approaches
discussed above remain robust in the presence of a small percentage of anomalies (less
than 10%). This is mainly because introducing a small fraction of anomalies
does not significantly affect the network’s model of normal behavior. For
scenarios where anomalies are known to occur sparingly, our experiments show that it’s possible to
relax the requirement of assembling a dataset consisting only of labeled normal samples for
training.</p>
<h4 id="discretizing-data-and-handling-stationarity">Discretizing Data and Handling Stationarity</h4>
<p>To apply deep learning approaches for anomaly detection (as with any other
task), we need to construct a dataset of training samples. For problem spaces
where data is already discrete, we can use the data as is (e.g., a dataset of
images of wall panels, where the task is to find images containing abnormal
panels). When data exists as a time series, we can construct our dataset by
discretizing the series into training samples. Typically this involves slicing
the data into chunks with comparable statistical properties. For example, given
a series of recordings generated by a data center temperature sensor, we can
discretize the data into daily or weekly time slices and construct a dataset
based on these chunks. This becomes our basis for anomaly comparison (e.g., the
temperature pattern for today is anomalous compared to patterns for the last 20
days). The choice of the discretization approach (daily, weekly,
averages, etc.) will often require some domain expertise; the one
requirement is that each discrete sample be comparable. For example, given that
temperatures may spike during work hours compared to non-work hours in the scenario
we’re considering, it may be challenging to discretize this data by hour as
different hours exhibit different statistical properties.</p>
<figure><img src="figures/ill-11.png" alt="Temperature readings for a data center overseveral days can be discretized (sliced) into daily 24-hour readingsand labeled (0 for a normal average daily temperature, 1 for an abnormal temperature) toconstruct a dataset."><figcaption>Temperature readings for a data center over
several days can be discretized (sliced) into daily 24-hour readings
and labeled (0 for a normal average daily temperature, 1 for an abnormal temperature) to
construct a dataset.</figcaption></figure>
<p>This notion of constructing a dataset of comparable samples is related to the
idea of <em>stationarity</em>. A stationary series is one in which properties of the data
(mean, variance) do not vary with time. Examples of non-stationary data include
data containing trends (e.g., rising global temperatures) or with seasonality
(e.g., hourly temperatures within each day). These variations need to be handled
during discretization. We can remove trends by applying a differencing function
to the entire dataset. To handle seasonality, we can explicitly include
information on seasonality as a feature of each discrete sample;for instance, to
discretize by hour, we can attach a categorical variable representing the hour of
the day. A common misconception regarding the application of neural networks capable of modeling temporal relationships such as LSTMs is that they automatically learn/model properties of the data useful for
predictions (including trends and seasonality). However, the extent to which
this is possible is dependent on how much of this behavior is represented in
each training sample. For example, to automatically account for trends or patterns
across the day, we can discretize data by hour with an additional categorical
feature for hour of day, or discretize by day (24 features for each hour).</p>
<p>Note: For most machine learning algorithms, it is a requirement that samples be <em>independent</em>
and <em>identically distributed</em>. Ensuring we construct comparable samples (i.e., handle
trends and seasonality) from time series data allows us to satisfy the latter
requirement, but not the former. This can affect model
performance. In addition, constructing a dataset in this way raises the possibility that
the learned model may perform poorly in predicting output values that
lie outside the distribution (range of values) seen during training�i.e.,
if there is a distribution shift. This greatly amplifies the need to
retrain the model as new data arrives, and complicates the model deployment process.
In general, discretization should be applied with care.</p>
<h3 id="selecting-a-model">Selecting a Model</h3>
<p>There are several factors that can influence the primary approach taken when it
comes to detecting anomalies. These include the data properties (time series vs.
non-time series, stationary vs. non-stationary, univariate vs. multivariate,
low-dimensional vs. high-dimensional), latency requirements, uncertainty
reporting, and accuracy requirements. More importantly, deep learning methods
are not always the best approach! To provide a framework for navigating this
space, we offer the following recommendations:</p>
<h4 id="data-properties">Data Properties</h4>
<ul>
<li>
<p><strong>Time series data</strong>: As discussed in the previous section, it is important to correctly discretize
the data and to handle stationarity before training a model. In addition, for discretized data with temporal relationships, the use of LSTM layers as part of the encoder or decoder can help model these relationships.</p>
</li>
<li>
<p><strong>Univariate vs Multivariate</strong>: Deep learning methods are well suited to data that has a wide range of features:
they’re recommended for high-dimensional data, such as images, and work well for
modeling the interactions between multiple variables. For most
univariate datasets, linear models<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> are both fast and accurate and thus typically preferred.
<strong>// RH: That’s a lot of acronyms to throw at the reader (in the footnote). It would be helpful to point them to a reference they can look at to explain them all (“For a survey of linear models, see…”). Keep the formatting consistent with refs in earlier footnotes or just include a URL.</strong></p>
</li>
</ul>
<h4 id="business-requirements">Business Requirements</h4>
<ul>
<li>
<p><strong>Latency</strong>: Deep learning models are slower than linear models. For scenarios that
include high volumes of data and have low latency requirements, linear models are
recommended (e.g., for detecting anomalies in authentication requests for
200,000 work sites, with each machine generating 500 requests per second).</p>
</li>
<li>
<p><strong>Accuracy</strong>: Deep learning approaches tend to be robust, providing better accuracy, precision,
and recall.</p>
</li>
<li>
<p><strong>Uncertainty</strong>: For scenarios where it is a requirement to provide a principled estimate of
uncertainty for each anomaly classification, deep learning models such as VAEs and BiGANs are recommended.</p>
</li>
</ul>
<h3 id="general-considerations-in-selecting-a-deep-learning-approach">General Considerations in Selecting a Deep Learning Approach</h3>
<p>When the (discretized) data contains sequences with temporal dependencies, a
sequence to sequence model can model these relationships, yielding better
results. For scenarios requiring principled estimates of uncertainty, a VAE and
GAN based approaches are suitable. For scenarios where the data is images, AE’s
VAEs and GANs designed with convolution blocks are suitable.</p>
<figure><img src="figures/ill-12.png" alt="The steps for selecting an approach to anomalydetection."><figcaption>The steps for selecting an approach to anomaly
detection.</figcaption></figure>
<p><strong>// RH: Add intro to table (something like “The following table highlights the pros and cons of the different types of models, to give you an idea of what kinds of data they are most useful for.”?) Also, in the GAN row of the table, what do you mean by “learning of data manifold”? And is it necessary to say “(useful for high-dimensional image data)” in item 2, given item 3?</strong></p>
<p>The following table highlights the pros and cons of the different types of models, to give you an idea under what kind of scenarios they are recommended.</p>
<div style="width: 100%; overflow-x: auto;"><table>
<thead>
<tr>
<th>Model</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>AutoEncoder</td>
<td><ul><li>Flexible approach to modeling complex non-linear patterns in data</li></td>
<td><ul><li>Does not support variational inference (estimates of uncertainty)</li><li>Requires a large dataset for training</li></ul></td>
</tr>
<tr>
<td>Variational AutoEncoder</td>
<td><ul><li>Supports variational inference (probabilistic measure of uncertainty)</li></ul></td>
<td><ul><li>Requires a large amount of training data, training can take a while</li></td>
</tr>
<tr>
<td>GAN (BiGAN)</td>
<td><ul><li>Supports variational inference (probabilistic measure of uncertainty) </li><li>Use of discriminator signal allows better learning of data manifold (useful for high dimensional image data).</li><li>Performs well for  high dimensional data (images) </li><li>GANs trained in semi-supervised learning mode have shown great promise, even with very few labeled data<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></li></ul></td>
<td><ul><li>Requires a large amount of training data, and longer training time (epochs) to arrive at stable results<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> </li><li>Training can be unstable (GAN mode collapse)</li></ul></td>
</tr>
<tr>
<td>Sequence to Sequence Model</td>
<td><ul><li>Well suited for data with temporal components (e.g., discretized time series data)</li></ul></td>
<td><ul><li>Slow inference (compute scales with sequence length which needs to be fixed)</li><li>Training can be slow</li><li>Limited accuracy when data contains features with no temporal dependence</li><li>Supports variational inference (probabilistic measure of uncertainty)</li></ul></td>
</tr>
<tr>
<td>One Class SVM</td>
<td><ul><li>Does not require a large amount of data</li><li>Fast to train</li><li>Fast inference time</li></ul></td>
<td><ul><li>Limited capacity in capturing complex relationships within data</li><li>Requires careful parameter selection (kernel, nu, gamma) that need to be carefully tuned.</li><li>Does not model a probability distribution, harder to compute estimates of confidence.</li></ul></td>
</tr>
</tbody>
</table></div>
<h2 id="prototype">Prototype</h2>
<p>In this section, we provide an overview of the data and experiments used to evaluate each of the approaches mentioned in the &lt;<technical>&gt; chapter. We also introduce two prototypes we built to demonstrate results from the experiments and how we designed each prototype.</p>
<h3 id="datasets">Datasets</h3>
<h4 id="kdd">KDD</h4>
<p>The  <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html">KDD network intrusion dataset</a> is a dataset of TCP connections that have been labelled as normal or representative of network attacks.</p>
<blockquote>
<p>A connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol.”</p>
</blockquote>
<p>These attacks fall into four main categories - denial of service, unauthorized access from a remote machine, unauthorized access to local superuser privileges, and surveillance e.g. port scanning.  Each TCP connection is represented as a set of attributes or features (derived based on domain knowledge) pertaining to each connection such as the number  of failed logins, connection duration, data bytes from source to destination etc. The dataset is comprised of a training set (97278 normal traffic samples, 396743 attack traffic samples ) and a test set (63458 normal packet samples, 185366 attack traffic samples). To make the data more realistic, the test portion of the dataset contains 14 additional attack types that are not in the train portion; thus, a good model should generalize well and detect attacks unseen at during training.</p>
<h4 id="ecg5000">ECG5000</h4>
<p>The  <a href="http://www.timeseriesclassification.com/description.php?Dataset=ECG5000">ECG5000</a> contains examples of ECG signals from a patient. Each data sample, which corresponds to an extracted heartbeat containing 140 points, has been labelled as normal or being indicative of heart conditions related to congestive heart failure. Given an ECG signal sample, the task is to predict if it is normal or abnormal. ECG5000 is well suited to a prototype for a few reasons — it is visual (signals can be visualized easily) and it is based on real data associated with a concrete use case (heart disease detection). While the task itself is not extremely complex, the data is multidimensional (140 values per sample which allows us demonstrate the value of a deep model), but small enough to rapidly train and run inference.</p>
<h3 id="benchmarking-experiment-setup">Benchmarking Experiment Setup</h3>
<p>We sought to compare each of the models discussed earlier using the KDD dataset. We preprocessed the data to keep only 18 continuous features (for easy reproducibility). Feature scaling (0-1 minmax scaling) is also applied to the data; scaling parameters are learned from training data and then applied to test data. We then trained each model using normal samples (97,278 samples) and evaluated it on the entire training set.</p>
<!-- a randomly selected subset of the test data (8,000 normal samples, and 2,000 abnormal samples).  -->
<p>We implemented each model using comparable parameters (see table below) that allow us to benchmark them in terms of training (mean time per training epoch, mean training time to best accuracy), inference (mean inference time), storage (size of weights, number of parameters), and performance (ROC, precision, recall). The deep learning models (AE, VAE, Seq2seq, BiGAN) were implemented in Tensorflow (keras api); each model was trained till best accuracy measured on the same validation dataset, using the <a href="https://keras.io/optimizers/">Adam optimizer</a> and a learning rate of  0.01.  OCSVM was implemented using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html">Sklearn OCSVM</a> library using the <em>rbf</em> kernel and parameters (<em>nu</em>=0.01 and <em>gamma</em>=0.5).  Additional details on the parameters for each model are summarized in the table below for reproducibility. These experiments were run on an Intel® Xeon® CPU @ 2.30GHz</p>
<p>Table …</p>
<h4 id="observations">Observations</h4>
<h5 id="training">Training</h5>
<p>In terms of training, ocsvm and an autoencoder were the fastest to implement and train. While the BiGAN based model also demonstrated very good accuracy, it was the most challenging to train. This is in part due to a known stability issue (mostly fluctuating accuracy during training) <sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> associated with GANs. Overall, the BiGAN approach, required more training epochs to arrive at stable results compared to the other deep methods.</p>
<!-- In terms of training time, we found that ocsvm and the autoencoder were the fastest models to train to peak accuracy. GANs have known stability issues ^[
[Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)]   and can be challenging to train. Overall, the BiGAN approach, required more training epochs to arrive at stable results compared to the other deep methods.  -->
<h5 id="inference-and-storage">Inference and Storage</h5>
<p>Being the least complex model, OCSVM had the fastest inference time (12x faster than an autoencoder). In terms of storage, the biGAN model has the largest number of parameters and overall size of weights. These values have implications for the number of concurrent models</p>
<h5 id="performance">Performance</h5>
<p>For the current KDD dataset, we found that the autoencoder had the best performance.</p>
<h3 id="web-application-prototypes">Web Application Prototypes</h3>
<p>We built two prototypes that demonstrate results and insights from our experiments. The first prototype – is built on on the KDD dataset used in the experiments above and is a visualization of the performance of 4 approaches to anomaly detection. The second prototype is an interactive explainer that focuses on the autoencoder model and results from applying it to detecting anomalies in ECG data.</p>
<h4 id="prototype-i---grant-to-add-prototype-name">Prototype I - Grant to Add Prototype Name</h4>
<p>This prototype is built on the KDD network intrusion dataset</p>
<h4 id="prototype-ii---anomagram">Prototype II - Anomagram</h4>
<p>This section describes Anomagram - an interactive web based experience where the user can build, train and evaluate an autoencoder to detect anomalous ECG signals. It utilizes the ECG5000 dataset mentioned above &lt;&lt;&gt;&gt;.</p>
<h5 id="ux-goals-for-anomagram">UX Goals for Anomagram</h5>
<p>Anomagram is designed as part of a growing area interactive visualizations (see Neural Network Playground [3], ConvNet Playground, GANLab, GAN dissection, etc) that help communicate technical insights on how deep learning models work. It is entirely browser based and  implemented in Tensorflow.js. This way, users can explore live experiments with no installations required. Importantly, Anomagram moves beyond the user of toy/synthetic data and situates learning within the context of a concrete task (anomaly detection for ECG data). The overall user experience goals for Anomagram are summarized as follows.</p>
<p>Goal 1: Provide an introduction to Autoencoders and how they can be applied to the task of anomaly detection. This is achieved via the <em>Introduction</em> module (see screenshot below). This entails providing definitions of concepts (reconstruction error, thresholds etc) paired with interactive visualizations that demonstrate concepts (e.g. an interactive visualization for inference on test data, a visualization of the structure of an autoencoder, a visualization of error histograms as training progresses, etc). fd</p>
<figure><img src="figures/anomagram-1.png" alt="Introduction module view."><figcaption><em>Introduction</em> module view.</figcaption></figure>
<p>Goal 2: Provide an interactive, accessible experience that supports technical learning by doing. This is mostly accomplished within the <em>Train a Model</em> module (see screenshot below)  and is designed for users interested in additional technical depth. It entails providing a direct manipulation interface that allows the user to specify a model (add/remove layers and units within layers), modify model parameters (training steps, batchsize, learning rate, regularizer, optimizer, etc), modify training/test data parameters (data size, data composition), train the model, and evaluate model performance (visualization of accuracy, precision, recall, false positive, false negative, ROC etc metrics) as each parameter is changed. Who should use Anomagram? Anyone interested in an accessible way to learn about autoencoders and anomaly detection . Useful for educators (tool to support guided discussion of the topic), entry level data scientists, and non-ML experts (citizen data scientists, software developers, designers etc).</p>
<figure><img src="figures/anomagram-2.png" alt="Train a Model module view."><figcaption><em>Train a Model</em> module view.</figcaption></figure>
<h5 id="interface-affordances-and-insights">Interface Affordances and Insights</h5>
<p>This section discusses some explorations the user can perform with Anomagram, and some corresponding insights.</p>
<p><strong>Craft (Adversarial) Input</strong>: Anomalies by definition can take many different and previously unseen forms. This makes the assessment of anomaly detection models more challenging. Ideally, we want the user to conduct their own evaluations of a trained model e.g. by allowing them to upload their own ECG data. In practice, this requires the collection of digitized ECG data with similar preprocessing (heartbeat extraction) and range as the ECG5000 dataset used in training the model. This is challenging. The next best way to allow testing on examples contributed by the user is to provide a simulator — hence the draw your ECG data feature. This provides a (html) canvas on which the user can draw signals and observe the model’s behaviour. Drawing strokes are converted to an array, with interpolation for incomplete drawings (total array size=140) and fed to the model. While this approach has limited realism (users may not have sufficient domain expertise to draw meaningful signals), it provides an opportunity to craft various types of (adversarial) samples and observe the model’s performance.
Insights: The model tends to expect reconstructions that are close to the mean of normal data samples. Using the Draw your ecg data feature, the user can draw (adversarial) examples of input data and observe model predictions/performance.</p>
<figure><img src="figures/anomagram-3.png" alt="Using the Draw your ecg data feature, the user can draw (adversarial) examples of input data and observe model predictions/performance."><figcaption>Using the Draw your ecg data feature, the user can draw (adversarial) examples of input data and observe model predictions/performance.</figcaption></figure>
<p><strong>Visually Compose a Model</strong>: Users can intuitively specify an autoencoder architecture using a direct manipulation model composer. They can add layers and add units to layers using clicks. This architecture is then used to specify the model’s parameters each time the model is compiled. This follows a similar approach used in “A Neural Network Playground”[3]. The model composer connector lines are implemented using the leaderline library. Relevant lines are redrawn or added as layers are added or removed from the model. Insights: There is no marked difference between a smaller model (1 layer) and a larger model (e.g. 8 layers) for the current task. This is likely because the task is not especially complex (a visualization of PCA points for the ECG dataset suggests it is linearly separable). Users can visually compose the autoencoder model — add remove layers in the encoder and decoder. To keep the encoder and decoder symmetrical, add/remove operations on either is mirrored.</p>
<p><strong>Effect of Learning Rate, Batchsize, Optimizer, Regularization</strong>: The user can select from 6 optimizers (Adam, Adamax, Adadelta, Rmsprop, Momentum, Sgd), various learning rates, and regularizers (l1, l2, l1l2).
Insights: Adam reaches peak accuracy with less steps compared to other optimizers. Training time increases with no benefit to accuracy as batchsize is reduced (when using Adam). A two layer model will quickly overfit on the data; adding regularization helps address this to some extent. Try them out!</p>
<p><strong>Effect of Threshold Choices on Precision/Recall</strong>: Earlier in this report (see background section on &lt;<Is Accuracy Enough>&gt;) we highlight the importance of metrics such as precision and recall and why accuracy is not enough. To support this discussion, the user can visualize how threshold choices impact each of these metrics.
Insights: As threshold changes, accuracy can stay the same but, precision and recall can vary. This further illustrates how the threshold can be used by an analyst as a lever to reflect their precision/recall preferences.</p>
<p><strong>Effect of Data Composition</strong>: We may not always have labelled normal data to train a model. However, given the rarity of anomalies (and domain expertise), we can assume that unlabelled data is mostly comprised of normal samples. However, this assumption raises an important question - does model performance degrade with changes in the percentage of abnormal samples in the dataset? In the train a model section, you can specify the percentage of abnormal samples to include when training the autoencoder model.
Insights: We see that with 0% abnormal data, the model AUC is ~96%. Great! At 30% abnormal sample composition, AUC drops to ~93%. At 50% abnormal data points, there is just not enough information in the data that allows the model to learn a pattern of normal behaviour. It essentially learns to reconstruct normal and abnormal data well and mse is no longer a good measure of anomaly. At this point, model performance is only slightly above random chance (AUC of 56%).</p>
<h2 id="landscape">Landscape</h2>
<p>In this section, we review the landscape of open source tools, service vendor
offerings, their trade-offs, and when to use each.</p>
<h3 id="open-source-tools-and-frameworks">Open Source Tools and Frameworks</h3>
<p>Popular open source machine learning libraries or packages in Python and R
include implementations of algorithmic techniques that can be applied to anomaly
detection tasks.  Algorithms (e.g., clustering, OC-SVM, isolation forests) exist
as part of a general-purpose framework like scikit-learn and do not cater
specifically to anomaly detection. In addition, generic packages for univariate
time series forecasting (e.g., Facebook’s
<a href="https://facebook.github.io/prophet/">Prophet</a>) have been applied widely to
anomaly detection tasks where anomalies are identified based on the difference
between the true value and a forecast.</p>
<p>In this section of the report, our focus is on comprehensive toolboxes that
specifically address the task of anomaly detection.</p>
<h4 id="python-outlier-detection-(pyod)">Python Outlier Detection (PyOD)</h4>
<p><a href="https://github.com/yzhao062/pyod">PyOD</a> is an open-source Python toolbox for performing scalable outlier detection
on multivariate data. Uniquely, it provides access to a wide range of outlier
detection algorithms, including established outlier ensembles and more recent
neural network-based approaches, under a single, well-documented API.</p>
<p>Distinct advantages:</p>
<ul>
<li>It contains more than 20 algorithms which cover both classical techniques such
as local outlier factor and recent neural network architectures such as
autoencoders or adversarial models.</li>
<li>PyOD implements combination methods for merging the results of multiple
detectors and outlier ensembles which are an emerging set of models.</li>
<li>It includes a unified API, detailed documentation, and interactive examples
across all algorithms for clarity and ease of use.</li>
<li>All models are covered by unit testing with cross-platform continuous
integration, code coverage, and code maintainability checks.</li>
<li>Optimization instruments are employed when possible: just-in-time (JIT)
compilation and parallelization are enabled in select models for scalable
outlier detection. Lastly, PyOD is compatible with both Python 2 and 3 across
major operating systems.</li>
</ul>
<h4 id="seldon%E2%80%99s-anomaly-detection-package">Seldon’s Anomaly Detection Package</h4>
<p><a href="https://github.com/SeldonIO">Seldon.io</a> is known for its open source machine learning deployment solution for
Kubernetes, which can in principle be used to serve arbitrary models. In
addition, the Seldon team has recently released
<a href="https://github.com/SeldonIO/alibi-detect">alibi-detect</a> - a Python package
focused on outlier, adversarial, and concept drift detection. The package aims
to cover both online and offline detectors for tabular data, text, images and
time series. The outlier detection methods should allow the user to identify
global, contextual and collective outliers.</p>
<p>They have identified anomaly detection as a sufficiently important capability to
warrant a dedicated attention in the framework, and have implemented several
models to use “out-the-box.” The existing models include seq2seq LSTMs,
variational auto-encoders, spectral residual for time series, gaussian mixture
models, isolation forests, Mahalanobis distance, and others. Note that they also
provide examples and documentation on how to use along with their platform.</p>
<p>In the Seldon Core architecture, anomaly detection methods may be implemented as
either a model or an input transformer. In the latter case, they can be composed
with other data transformations to process inputs to another model. This nicely
illustrates one role anomaly detection can play in machine learning systems:
flagging bad inputs before they pass through the rest of a pipeline.</p>
<h4 id="r-packages">R packages</h4>
<p>The following section reviews R packages that have been created for anomaly
detection. Interestingly, most of them deal with time series data.</p>
<h5 id="twitter%E2%80%99s-anomalydetection-package">Twitter’s AnomalyDetection package</h5>
<p>Twitter’s <a href="https://github.com/twitter/AnomalyDetection">AnomalyDetection</a> is an open-source R package to automatically detect
anomalies. It is applicable across a wide variety of contexts (for example,
detecting anomalies in system metrics after a new software release, user
engagement after an A/B test, or problems in econometrics, financial
engineering, political and social sciences). It can help detect global/local
anomalies as well as positive/negative (i.e., a point-in-time increase/decrease
in values) anomalies.</p>
<p>The primary algorithm, Seasonal Hybrid ESD (S-H-ESD), builds upon the
Generalized ESD test for detecting anomalies - which can be global as well as
local. This is achieved by employing time series decomposition and using robust
statistical metrics, i.e., median together with ESD. In addition, for a long
time series (say, 6 months of minutely data), the algorithm employs piecewise
approximation.</p>
<p>Besides time series, the package can also be used to detect anomalies in a
vector of numerical values when the corresponding timestamps are not available.
The package provides rich visualization support. The user can specify the
direction of anomalies, the window of interest (such as last day, last hour), as
well as enable/disable piecewise approximation, and the x- and y-axis are
annotated to assist visual data analysis.</p>
<h5 id="anomalize-package">Anomalize package</h5>
<p>The <a href="https://github.com/business-science/anomalize">anomalize</a> package, open sourced by Business Science, does time series
anomaly detection that goes inline with other <a href="https://www.tidyverse.org/">Tidyverse
packages</a> (or packages
supporting tidy data).</p>
<p>Anomalize has three main functions:</p>
<ul>
<li>Decompose: separates out the time series into seasonal, trend, and remainder
components</li>
<li>Anomalize: applies anomaly detection methods to the remainder component</li>
<li>Recompose: calculates upper and lower limits that separate the “normal” data
from the anomalies</li>
</ul>
<h5 id="tsoutliers-package">Tsoutliers package</h5>
<p>This <a href="https://www.rdocumentation.org/packages/tsoutliers/versions/0.6-8">package</a>
implements a procedure based on the approach described in <a href="https://www.researchgate.net/publication/243768707_Joint_Estimation_of_Model_Parameters_and_Outlier_Effects_in_Time_Series">Chen and
Liu
(1993)</a>
for automatic detection of outliers in time series. Time series data
often undergoes non-systematic changes that alter the dynamics of the data
transitory or permanently. The approach considers innovational outliers,
additive outliers, level shifts, temporary changes, and seasonal level shifts -
while fitting a time series model.</p>
<h4 id="numenta%E2%80%99s-htm-(hierarchical-temporal-memory)">Numenta’s HTM (Hierarchical Temporal Memory)</h4>
<p>Research organization <a href="https://numenta.com/">Numenta</a> has introduced hierarchical temporal memory (HTM)
\– a machine learning model for anomaly detection. At the core of HTM are
time-based learning algorithms that store and recall temporal patterns. Unlike
most other machine learning methods, HTM algorithms learn time-based patterns in
unlabeled data on a continuous basis. They are robust to noise, and high
capacity - meaning they can learn multiple patterns simultaneously. The HTM
algorithms are documented and available through its open source project, NuPIC
(Numenta Platform for Intelligent Computing). The HTM technology is suited to
address a number of problems, particularly those with the following
characteristics: streaming data, underlying patterns in data change over time,
subtle patterns, and time-based patterns.</p>
<p>One of the first commercial applications developed using NuPIC is
<a href="https://grokstream.com/">Grok</a>, which
performs IT analytics, giving insight into IT systems to identify unusual
behavior and reduce business downtime. Another is
<a href="https://www.cortical.io/">Cortical.io</a>, which enables
applications in natural language processing.</p>
<p>The NuPIC platform also offers several tools such as the HTM Studio and the
Numenta Anomaly Benchmark (NAB). HTM Studio is a desktop tool that finds
anomalies in time series without the need to program, code, or set parameters.
NAB is a novel benchmark for evaluating and comparing algorithms for anomaly
detection in streaming, real-time applications. It is composed of over 50
labeled real-world and artificial time series data files, plus a novel scoring
mechanism designed for real-time applications.</p>
<p>Besides this, there are example applications available on NuPIC that include
sample code and white papers for: tracking anomalies in the stock market, ogue
behavior detection - finding anomalies in human behavior, geospatial tracking -
finding anomalies in objects moving through space and time</p>
<p>Numenta is a technology provider and does not create go-to-market solutions for
specific use cases. The company licenses their technology and application code
to developers, organizations, and companies who wish to build upon their
technology. Numenta has several different types of licenses, including open
source, trial, and commercial licenses. Developers can use Numenta technology
within NuPIC using the AGPL v3 open source license. Their HTM Studio is a free,
desktop tool allows you to test whether our Hierarchical Temporal Memory (HTM)
algorithms will find anomalies in your data without having to program, code or
set parameters.</p>
<h3 id="anomaly-detection-as-a-service">Anomaly Detection as a Service</h3>
<p>In this section, we survey a sample of anomaly detection service providers. Most
of these providers can access data from public cloud databases, provide some
kind of dashboard/report format to view/analyze data, have an alert mechanism
when an anomaly occurs, and view underlying causes. An anomaly detection
solution should try to reduce the time to detect and speed up the time to
resolution by identifying KPIs and attributes that are causing the alert.</p>
<h4 id="anodot">Anodot</h4>
<h5 id="capabilities">Capabilities</h5>
<p><a href="https://www.anodot.com/">Anodot</a> is a real-time analytics and automated anomaly detection system that
detects and
turns outliers in time series data into business insights. They explore anomaly
detection from the perspective of forecasting, where anomalies are identified
based on deviations from expected forecasts. As part of their product, they do
two things:</p>
<ul>
<li>Business monitoring: the system does SAAS monitoring, finds anomalies and helps
with root cause analysis</li>
<li>Business forecasting: forecasting, what if, and optimization.</li>
</ul>
<h5 id="data-requirements">Data requirements</h5>
<p>Anodot supports multiple input data sources including direct uploads, or
integrations with Amazon’s S3 or Google Cloud storage. They are data agnostic
and can track a variety of  metrics, e.g., revenue, number of sales, the number
of page visits, number of daily active users, etc.</p>
<h5 id="modeling-approach%2Ftechnique(s)">Modeling approach/technique(s)</h5>
<p>Anodot analyzes all the business metrics in real-time and at scale by running
its ML algorithms on the live data stream itself, without reading or writing
into a database. Every data point that flows into Anodot from all data sources
is correlated with the relevant metric’s existing normal model, and either
flagged as an anomaly or serves to update the normal model. They believe that no
single model can be used to cover all metrics. To allocate the optimal model for
each metric, they first create a library of model types for different signal
types (metrics that are stationary, non-stationary, multimodal, discrete,
irregularly sampled, sparse, stepwise, etc.). Each new metric goes through a
classification phase, and is matched with the optimal model. The model then
learns “normal behavior” for each metric, which is a prerequisite to identifying
anomalous behavior. To accommodate this kind of learning in real-time at scale,
they use sequential adaptive learning algorithms which initialize a model of
what is normal on the fly, and then compute the relation of each new data point
going forward.</p>
<h5 id="point-anomalies-or-intervals">Point anomalies or intervals</h5>
<p>Instead of flagging individual data points as anomalies, Anodot highlights
intervals. Points within the entire duration of the interval are considered
anomalous, preventing redundant alerts.</p>
<h5 id="thresholding">Thresholding</h5>
<p>While users can specify static thresholds which trigger alerts, Anodot also
provides automatic defaults where no thresholding input from the user is
required.</p>
<h5 id="root-cause-investigation">Root cause investigation</h5>
<p>Anodot helps investigate why the alert was triggered. It tries to understand how
different active anomalies correlate (to expedite root cause investigation) and
shortens the time to resolution. Anodot bands groups together different
anomalies (or “incidents”) which tell the story of the phenomena. These can be
(a) multiple anomalous values in the same dimension (e.g., a drop in traffic
from sources A and B, but not C, D or E), or (b) correlation between different
anomalous KPIs, such as visits, conversions, orders revenue, error rate. Each
incident has an Anomap, a graphic distribution of the dimensions most impacted.
This is essentially a heat map that makes it easier to understand the whole
picture.</p>
<h5 id="user-interface">User Interface</h5>
<p>The Anodot interface enables the user to visualize and explore alerts. With the
receipt of every alert, users are prompted to give the alert a binary score
(good catch / bad catch). This input is fed back into the learning model to
further tune it by providing real-life indications about the validity of its
performance. By training the algorithms with direct feedback on anomalies, users
can influence the system’s functionality and results.</p>
<h5 id="delivery">Delivery</h5>
<p>Notifications can be forwarded to every user through his/her choice of
channel(s). Anodot notification integrations include—but are not limited
to—Slack, API, email, pagerduty, Jira, Microsoft Teams OpsGenie, etc.</p>
<h4 id="amazon%E2%80%99s-quicksight">Amazon’s QuickSight</h4>
<h5 id="capabilities-2">Capabilities</h5>
<p>Amazon <a href="https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html">QuickSight</a>
is a cloud-native BI service that allows its users to create
dashboards and visualizations to communicate business insights. In winter 2019,
Amazon’s machine learning capability was integrated with QuickSight to provide
anomaly detection, forecasting, and auto-narrative capabilities as part of the
BI tool. It is a licensed software and pricing is usage-based; you only pay for
the active usage, regardless of the number of users. That said, the
pay-per-session could end up being expensive since anomaly detection tasks are
compute intensive</p>
<h5 id="data-requirements-2">Data requirements</h5>
<p>QuickSight requires you to connect or import structured data  directly query a
SQL-compatible source, or ingest the data into SPICE. There is a requirement on
the number of  historical data points varying based on the task (analyzing
anomalies or forecasting). There are also restrictions on the number of category
dimensions that could be included (for example, product category, region,
segment).</p>
<h5 id="modeling-approach%2Ftechnique(s)-2">Modeling approach/technique(s)</h5>
<p>QuickSight provides a point and click solution to learn anomalous behavior and
generate forecasts. It utilizes a built-in version of the Random Cut Forest
(RCF) online algorithm, which can not only  be noisy, but lead to large amounts
of false positive alerts. On the plus side, it provides a customizable
narrative feature that explains key takeaways from the insights generated. For
instance, it could provide a summary of how the revenue compares to a previous
period or a 30-day average, and/or highlight the event (in case of an anomaly).</p>
<h5 id="point-anomalies-or-intervals-2">Point anomalies or intervals</h5>
<p>Anomalous events are presented discretely, on a point-by-point basis. So if an
anomaly lasts more than a single time unit, the system will flag several events
which could be noisy and redundant.</p>
<h5 id="thresholding-2">Thresholding</h5>
<p>Anomaly detection with QuickSight employs a thresholding approach to trigger
anomalous events. The user provides a threshold value (low, medium, high, very
high) that determines how sensitive the detector is to detected anomalies.
Expect to see more anomalies when the setting is low, and fewer anomalies when
the setting is set to high. This sensitivity is determined based on standard
deviations of the anomaly score generated by the RCF algorithm. This approach
could be very tedious - especially when there are multiple time series being
analyzed across various data hierarchy combinations - and introduces manual
intervention.</p>
<h5 id="root-cause-investigation-2">Root cause investigation</h5>
<p>One can interactively explore the anomalies on the Quicksight dashboard/report
to help understand the root cause. QuickSight performs a contribution analysis
which highlights  the factors that significantly contribute to an anomaly. If
you have dimensions in your data that are not being used in the anomaly
detection, you can add up to four of them for contribution analysis. In
addition, it can support interactive “what-if” queries. In these, some of the
forecasts can be altered and treated as hypotheticals to provide conditional
forecasts.</p>
<h5 id="user-interface-2">User interface</h5>
<p>Quicksight anomaly detection provides a basic reporting interface. From a UI
perspective, it is fairly unexceptional. For instance, it lacks a way to
understand the overall picture with anomalous points. (Do the anomalies have
some common contributing factors?) Furthermore, the forecasted values do not
have confidence intervals associated with them, which would help the end-user
visually understand the magnitude of the anomaly. As it stands, there is no
basis for comparison.</p>
<h5 id="delivery-2">Delivery</h5>
<p>The QuickSight dashboards can be embedded within applications, shared among
users, and/or be sent via email - as long as the recipients have a QuickSight
subscription.</p>
<h4 id="outlier.ai">Outlier.ai</h4>
<h5 id="capabilities-3">Capabilities</h5>
<p><a href="https://outlier.ai/">Outlier.ai</a> is a licensed software that uses artificial intelligence to automate
the process of business analytics. It can connect to databases provided by cloud
services and automatically provides insights to your inbox - without the need to
create reports or write queries. Outlier works with customers across industry
segments, applying machine learning to automatically serve up business critical
insights.</p>
<h5 id="data-requirements-3">Data requirements</h5>
<p>Outlier.ai can connect to databases provided by cloud services.</p>
<h5 id="modeling-approach%2Ftechnique(s)-3">Modeling approach/technique(s)</h5>
<p>Unknown</p>
<h5 id="point-anomalies-or-intervals-3">Point anomalies or intervals</h5>
<p>Anomalous events are presented discretely, on a point-by-point basis.</p>
<h5 id="thresholding-3">Thresholding</h5>
<p>N/A</p>
<h5 id="root-cause-investigation-3">Root cause investigation</h5>
<p>Outlier.ai allows customers to not only surface key insights about business
changes automatically, but also identify the likely root causes of those
changes; this feature guides teams in making quick, informed business decisions.
Teams can easily share stories through PDF, PowerPoint optimized images, or an
auto-generated email, annotated with their comments.</p>
<h5 id="user-interface-3">User interface</h5>
<p>The UI is very similar to most standard BI tools, making it fairly
user-friendly.</p>
<h5 id="delivery-3">Delivery</h5>
<p>The dashboards can be embedded within applications, shared among users, and/or
be sent via email.</p>
<h4 id="vectra.ai">Vectra.ai</h4>
<h5 id="capabilities-4">Capabilities</h5>
<p>In simple terms, Vectra’s flagship platform, <a href="https://www.vectra.ai/">Cognito</a> can be described as an
intrusion detection system. This cloud-based, network detection-and-response
system performs a number of cybersecurity related tasks including network
traffic monitoring and anomaly detection. For this latter task, metadata
collected from captured packets (rather than via deep-packet inspection) is
analyzed using a range of behavioral detection algorithms. This provides
insights about outlier characteristics that can be applied in a wide range of
cybersecurity detection-and-response use cases. Cognito works with both
encrypted and unencrypted traffic.</p>
<h5 id="data-requirements-4">Data requirements</h5>
<p>It can connect to databases provided by cloud services. Cognitio also uses
metadata drawn from Active Directory and DHCP logs.</p>
<h5 id="modeling-approach%2F-technique(s)">Modeling approach/ technique(s)</h5>
<p>According to a
<a href="https://content.vectra.ai/rs/748-MCE-447/images/WhitePaper_2019_The_data_science_behind_Cognito_AI_threat_detection_models_English.pdf?mkt_tok=eyJpIjoiWkRGaVpHVmtaVGxrTkdFeiIsInQiOiJ2RVhkK3M0cHU3dXNQRDZ2YnA3QW16K0ZKVFVEK1lDeFRwcTZPMGxXZlB0clhOYmhPaVBXenkzRmY1Ylwvakp5d2FcL1dSakVKbDZhcHZtNEdZU1A3aHFMYkpxVlZHWXllXC9xUGRPOXNtZ0NyTFRjTitxUlVkaXBzNFdiQlBaUUxwVSJ9">whitepaper</a>
published by Vectra, a mix of ML approaches are used
to deliver the cybersecurity features the platform supports, in both global and
local (network) contexts. For example, supervised learning techniques (such as
Random Forest) can help to address cyberthreats associated with suspicious HTTP
traffic patterns. Drawing on large-scale analysis of many types of malicious
traffic and content as well as domain expertise, RF can be used to identify
patterns of command-and-control behavior that don’t exist in benign HTTP
traffic.
Vectra also uses unsupervised ML techniques such as k-means clustering to
identify valid credentials that have potentially been stolen from a compromised
host. This type of theft is the basis of cyberattacks such as pass-the-hash and
golden ticket. Elsewhere, deep learning has proven effective in detecting
suspicious domain activity; specifically, the detection of
algorithmically-generated domains that are set up by cyberattackers as the
front-end of their command-and-control infrastructure.</p>
<h5 id="point-anomalies-or-intervals-4">Point anomalies or intervals</h5>
<p>Anomalous events are presented discretely, on a point-by-point basis.</p>
<h5 id="thresholding-4">Thresholding</h5>
<p>The scoring of compromised hosts by the Vectra Threat Certainty Index allows
security teams to define threshold levels based on combined scoring.</p>
<h5 id="root-cause-investigation-4">Root cause investigation</h5>
<p>All detection events are correlated to specific hosts that show signs of threat
behaviors. In turn, all context is assimilated into an up-to-the-moment score of
the overall risk to the organization.</p>
<h5 id="user-interface-4">User interface</h5>
<p>Vectra.ai’s Cognito platform delivers detection information via a simple
dashboard which displays information such as: a prioritized (in terms of risk)
list of compromised hosts, changes in a host’s threat and certainty scores, and
‘key assets’ that show signs of attack.</p>
<h5 id="delivery-4">Delivery</h5>
<p>The platform supports information-sharing by security teams on demand, or on a
set schedule managed by its customizable reporting engine. Real-time
notifications about network hosts - with attack indicators that have been
identified (with the highest degree of certainty) as posing the biggest risk -
is also a supported feature.</p>
<h4 id="yahoo%E2%80%99s-anomaly-detector---sherlock">Yahoo’s Anomaly Detector - Sherlock</h4>
<h5 id="capabilities-5">Capabilities</h5>
<p><a href="https://github.com/yahoo/sherlock">Sherlock</a> is an open source anomaly detection
service built on top of <a href="http://druid.io/">Druid</a> (an
open-source, distributed data store). It leverages the <a href="https://github.com/yahoo/egads">Java library, Extensible
Generic Anomaly Detection System (EGADS)</a> to detect anomalies in time-series
data. It’s fast and scalable. It allows users to schedule jobs on an hourly,
daily, weekly, or monthly basis (although it also supports ad hoc real-time
anomaly detection requests). Anomaly reports can be viewed from Sherlock’s
interface, or received via email.</p>
<h5 id="data-requirements-5">Data requirements</h5>
<p>Sherlock accesses time series data via Druid JSON queries and uses a Redis
backend to store job metadata, the anomaly reports (and other information) it
generates, as well as a persistent job queue. These anomaly reports can be
accessed via direct requests using the Sherlock client API or delivered via
scheduled email alerts.</p>
<h5 id="modeling-approach%2F-technique(s)-2">Modeling approach/ technique(s)</h5>
<p>Sherlock takes a time-series modeling based approach to anomaly detection using
three important modules from the EGADS library - time series modelling, anomaly
detection, and alerting. The times-series modeling module supports the use of
historical data to learn trends and seasonality in the data using model classes
such as ARIMA. The resulting values are applied to the anomaly detection models
that comprise the Anomaly Detection module.  These models support a number of
detection scenarios that are relevant in a cybersecurity context (e.g., outlier
detection and change point detection). The Alerting module uses the error metric
produced from anomaly detection models and outputs candidate anomalies based on
dynamically learnt thresholds, learning to filter out irrelevant anomalies over
time.</p>
<h5 id="point-anomalies-or-intervals-5">Point anomalies or intervals</h5>
<p>Anomalous events are presented both discretely, on a point-by-point basis, and
as intervals.</p>
<h5 id="threshold">Threshold</h5>
<p>Thresholds are learnt dynamically. No thresholding input from the user is
required/supported.</p>
<h5 id="root-cause-analysis">Root cause analysis</h5>
<p>N/A</p>
<h5 id="user-interface-5">User interface</h5>
<p>Sherlock’s user interface is built with <a href="http://sparkjava.com/">Spark Java</a> -
a UI framework for building
web applications. The UI enables users to submit instant anomaly analyses,
create and launch detection jobs, and view anomalies on both a heatmap and a
graph.</p>
<h5 id="delivery-5">Delivery</h5>
<p>Scheduled anomaly requests are delivered via email or directly via API-based
queries.</p>
<h2 id="ethics">Ethics</h2>
<p>Machine learning models that learn to solve tasks independently from data are
susceptible to biases and other issues that may raise ethical concerns. Anomaly
detection models have specific risks and mitigation tactics. In anomaly
detection, when the definition of “normal” is independently learned and applied
without controls, it may inadvertently reflect societal biases that can lead to
harm. This presents risks to human welfare in scenarios where anomaly detection
applications intersect with human behavior. It should go without saying that in
relation to humans, we must resist the assumption that different is bad.</p>
<h3 id="diversity-matters">Diversity Matters</h3>
<p>Although anomalies are often scrutinized, their absence in the data may be even
worse! Consider a model that is trained on x-ray images of normal luggage
content that only uses images of luggage packed by citizens of North America.
This could lead to unusually high stop and search rates for users from other
parts of the world, where items packed might differ greatly.</p>
<p>To limit the potential harm of a machine learning model’s tendency to assume
that “different is bad,” we can use a larger (or more varied) dataset, and
provide human supervision of the model (both measures reduce the likelihood of
errors). In the above example, this translates to using more varied x-ray image
data to expand the machine’s view of what is normal, and using human review to
ensure that positive predictions aren’t false positives.</p>
<h3 id="explainability">Explainability</h3>
<p>In many anomaly detection applications, the system presents anomalous instances
to the end user - a decision maker to provide a verdict (label) which can then
be fed back to the model to refine it further. Unfortunately, some applications
may not provide enough explanation about why an instance was considered
anomalous, leaving the end user with no particular guidance on where to begin
investigation. Blindly relying on such applications may cause or in certain
cases exacerbate bias.</p>
<h3 id="additional-use-cases">Additional Use Cases</h3>
<p>Ethical considerations are important in every use of machine learning,
particularly when the use case affects humans. Below are a few use cases in
which this is particularly true of anomaly detection.</p>
<h4 id="data-privacy">Data Privacy</h4>
<p>Protecting an individual’s data has been a growing concern in the last few
years, culminating in recent data privacy laws like the GDPR and CCPA. These
laws don’t just consider and penalize data breaches, but also guide and limit
how an individual’s personal data can be used and processed. Thus, when anomaly
detection methods are used on protected data, privacy is paramount.</p>
<p>To give an example, in &lt;&lt;Chapter 3. Deep Learning for Anomaly Detection&gt;&gt; we
discussed autoencoders, a type of neural network that has been widely used for
anomaly detection. It contains an encoder network which reduces the dimension of
the input data, and a decoder network which aims to reconstruct the input. The
learning goal of autoencoders is to minimize the reconstruction error, which is
consequently the loss function. Because the dimensionality reduction brings
information loss, and the learning goal encourages to preserve the information
that is common to most training samples, anomalies that contain rare information
could be identified by measuring model loss.</p>
<p>In certain use cases, these identified anomalies could correspond to
individuals. Improper disclosure of such data can have adverse consequences for
a data subject’s private information, or even lead to civil liability or bodily
harm. One way to minimize these effects is to use a technique called
differential privacy<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> on the data before it is fed into an anomaly detection
system. This technique essentially adds a small amount of noise to the data, in
order to mask individual identities while maintaining the accuracy of aggregated
statistics. When coupled with an anomaly detection system, differential privacy
has been shown to reduce the false positives<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>, thus protecting the privacy of
more individuals who would otherwise have been scrutinized by being singled out.</p>
<h4 id="health-care-diagnostics">Health Care Diagnostics</h4>
<p>Anomaly detection can be applied in health care scenarios to provide quick and
early warnings for medical conditions. For example, a model can surface chest
x-rays that substantially differ from normal chest x-rays, or highlight images
of tissue samples as containing abnormalities. These analyses can have
tremendous consequences for the patient. A false negative means an undetected
disease, and a false positive means unnecessary - and potentially painful or
even harmful - treatment.</p>
<p>For other machine learning tasks (e.g., churn prediction, resume review), we
strive to remove racial or socioeconomic factors from the equation. In health
care diagnostics, it may be both appropriate and advantageous to focus on them.</p>
<p>To the extent that an anomaly to be detected is connected with a certain group
of people, models can be tailored to that group. For example, sickle-cell
disease is more prevalent in parts of Africa and Asia than in Europe and the
Americas. A diagnostic system for detecting this disease should include enough
samples of Asian and African patients and acknowledgement of their ethnicity to
make sure the disease is identified.</p>
<p>In any event, it is important to ensure that these systems remain curated (i.e.,
a medical professional verifies results) and that the models are correctly
evaluated (precision, recall) before being put into production.</p>
<h4 id="security">Security</h4>
<p>Similar to the luggage example we’ve already referenced, home or business
security systems trained to identify anomalies also present a problem of the
“different is bad” variety. These systems need to have enough data - in terms of
quantity and variability -  to prevent bias that would make them more likely to
identify people of different races or socioeconomic status as anomalies based
on, for example, their color or clothing.</p>
<h4 id="content-moderation">Content Moderation</h4>
<p>Blind reliance on anomaly detection systems for content moderation can lead to
false positives that limit or silence system users based on the language or
types of devices they use. Content moderation software should be monitored for
patterns of inappropriate blocking or reporting, and should have a user
appeal/review process.</p>
<h4 id="financial-services">Financial Services</h4>
<p>Determining what harmful anomalies are in financial services is complex. On one
hand, fraudsters are actively trying to steal and launder money, and move it
around the world. On the other hand, nearly all transactions are legitimate
services for valuable customers. Fraudsters affirmatively emulate normal
business, making anomalies especially difficult to identify. As a result,
financial services organizations should consider first whether anomaly detection
is desirable in their use cases, and then consider the potential ethical and
practical risks.</p>
<p>For example, banks use customer data to offer mortgages or student loans. A lack
of diverse data could lead to unfavorable results for certain demographic
groups. These biased algorithms can result in costly mistakes, reduce customer
satisfaction, and damage a brand’s reputation. To combat this, one should pose
questions (like the following) that can help check for bias in the data or
model:</p>
<ul>
<li>Fact check: do the detected anomalies mostly belong to underrepresented groups?</li>
<li>Are there enough features that explain minority groups?</li>
<li>Has model performance been evaluated for each subgroup within your data?</li>
</ul>
<h3 id="innovation">Innovation</h3>
<p>As we have seen, the term anomaly can carry negative connotations. The nature of
anomaly detection is identifying samples that are different from the bulk of the
other samples - but as discussed, assuming that “different is bad” is not
necessarily fair in many cases. Perhaps instead of associating faulty
interpretations with anomalies, pursuing them to reveal new truths could be
helpful.</p>
<p>After all, progress in science is often triggered by anomalous activities that
lead to innovation!</p>
<h2 id="conclusion">Conclusion</h2>
<p>Anomaly detection is a classic problem, common to many business domains. In this
report, we have explored how a set of deep learning approaches can be applied in
a semi-supervised setting for addressing the anomaly detection task. We think
this focus is useful for the following reasons:</p>
<ul>
<li>While deep learning has demonstrated superior performance for many business
tasks, there is fairly limited information on how anomaly detection can be cast
as a deep learning problem and how deep models perform.</li>
<li>A semi-supervised approach is desirable in handling unseen, unknown anomalies
and does not incur vast data labeling costs for businesses.</li>
<li>Traditional machine learning approaches are suboptimal for handling high
dimensional, complex data and modeling interactions between each variable.</li>
</ul>
<p>In our &lt;<prototype>&gt;, we show how deep learning models can achieve competitive
performance on a multivariate tabular dataset (network intrusion detection).
This is in concurrence with results from existing research that show superior
performance of deep learning models for high dimensional data such as images
(cite).</p>
<p>Overall, while deep learning approaches are useful, there are a few challenges
that may limit their deployment in production settings.</p>
<ul>
<li>
<p><strong>Latency:</strong> Compared to linear models (such as AR, ARMA, etc.) or shallow machine learning
models (such as One Class SVM), deep learning models can have significant
latency associated with inference. This makes it expensive to apply them within
streaming data (high volume, high velocity) use cases at scale. For example, our
experiments show that inference with OCSVM is 12x faster than with an
autoencoder.</p>
</li>
<li>
<p><strong>Data Requirements:</strong> Deep learning models typically require a large dataset (tens of thousands of
samples) for effective training. Further, deep models are prone to overfitting
and need to be carefully evaluated to address this. Many anomaly use cases can
frequently have few data points (e.g., daily sales data for two years will
generate 712 samples, which may be insufficient to train a model). In such
scenarios, linear models designed to work with smaller datasets are a better
option.</p>
</li>
<li>
<p><strong>Managing Distribution Shift:</strong> In many scenarios, the underlying process generating data may legitimately
change such that a datapoint that was previously anomalous becomes normal. The
changes could be gradual, cyclical, or even abrupt in nature. This phenomenon,
although not unique to anomaly detection, is known as concept drift. With
respect to anomaly detection, one way to handle this is to frequently retrain
the model as new data arrives - or trigger retraining when concept drift is
detected. It can be challenging to maintain this continuous learning approach
for deep models, as training time can be significant.</p>
</li>
</ul>
<p>Going forward, we expect the general approaches discussed in the report to
continually evolve and mature. As examples, see recent extensions to the
encoder-decoder model approach that are based on GMMs<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>, LSTMs<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>,
CNNs<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>, and GANs<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>.
Like everything else in machine learning, there is no “one size fits all”; no
one model works best for every problem. The right approach always depends on the
use case and data.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>
<a href="https://arxiv.org/abs/1906.02694">Deep Semi-Supervised Anomaly Detection</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://dl.acm.org/doi/10.1145/1541880.1541882">Anomaly Detection, A Survey by Chandola et al 2009</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://arxiv.org/abs/1705.07874">A Unified Approach to Interpreting Model
Predictions</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Approaches such as AR, MA, ARMA, ARIMA,
SARIMA, and VAR models. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://arxiv.org/abs/1901.03407">Deep Learning for Anomaly Detection: A Survey</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf">Improved Techniques for Training GANs</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/abs/1606.03498">Improved Techniques for Training GANs</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Differential Privacy, Dwork 2006 <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://arxiv.org/abs/1911.07116">Robust Anomaly Detection and
Backdoor Attack Detection Via Differential
Privacy</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><a href="https://openreview.net/forum?id=BJJLHbb0-"> Deep Autoencoding
Gaussian Mixture Model for Unsupervised Anomaly
Detection</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://arxiv.org/abs/1910.03818">Sequential
VAE-LSTM for Anomaly Detection on Time Series</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="https://arxiv.org/abs/1906.03821">Time-Series Anomaly Detection Service at
Microsoft</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://arxiv.org/abs/1901.04997">MAD-GAN: Multivariate
Anomaly Detection for Time Series Data with Generative Adversarial
Networks</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

        </div>
      </body>
    </html>
  